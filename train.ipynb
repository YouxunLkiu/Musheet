{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f80e4451",
   "metadata": {},
   "source": [
    "# Piano to Sheet\n",
    "\n",
    "Converting .wav piano pieces into music sheets.\n",
    "\n",
    "I\n",
    "\n",
    "## Pipeline break down ( beta)\n",
    "\n",
    "1. Loading in the wave file,\n",
    "2. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941fd603",
   "metadata": {},
   "source": [
    "## Audio Processing  Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaff679",
   "metadata": {},
   "source": [
    "For the processing libraries, we will be using pretty_midi to extract out label information from the given midi files.  \n",
    "And for processing our wav audio files. We will be using librosa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c76f67",
   "metadata": {},
   "source": [
    "## Machine Learning Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85047046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import librosa \n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pretty_midi as pm\n",
    "import mido\n",
    "import IPython.display as ipd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a26ef70",
   "metadata": {},
   "source": [
    "# Model Saving funtion and Model loading function\n",
    "\n",
    "Having a model saving function and a modle loading function in which we can train the model in small epoch progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cccab19",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef91a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_file_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/maestro-v3.0.0.json\"\n",
    "with open(labels_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "all_sets = {}\n",
    "all_sets['train'] = []\n",
    "all_sets['validation'] = []\n",
    "all_sets['test'] = []\n",
    "\n",
    "def sortingsets (data, allsets):\n",
    "    for key in data:\n",
    "       \n",
    "        if data[key] == 'train':\n",
    "            all_sets['train'].append(key)\n",
    "        elif data[key] == 'validation':\n",
    "            all_sets['validation'].append(key)\n",
    "        else:\n",
    "            all_sets['test'].append(key)\n",
    "\n",
    "def save_index_to_csv(all_sets):\n",
    "    for key in all_sets:\n",
    "        path = f\"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/{key}_indicies.csv\"\n",
    "        df = pd.DataFrame({'Index': all_sets[key]})\n",
    "        df.to_csv(path, index=False)\n",
    "\n",
    "def load_index_from_csv(path):\n",
    "    indices = []\n",
    "    with open(path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            indices.append(int(row[0]))\n",
    "    return indices\n",
    "\n",
    "def save_progress_index_to_csv(indices, epoch):\n",
    "    path = f\"models/training_index{epoch}.csv\"\n",
    "    df = pd.DataFrame({'Index': indices})\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "def load_progress_index_from_csv(epoch):\n",
    "    path = f\"models/training_index{epoch}.csv\"\n",
    "    indices = []\n",
    "    with open(path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            indices.append(int(row[0]))\n",
    "    return indices\n",
    "\n",
    "sortingsets(data['split'], all_sets)\n",
    "save_index_to_csv(all_sets)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1870b3aa",
   "metadata": {},
   "source": [
    "traindata/maestro-v3.0.0-midi/maestro-v3.0.0/train_indicies.csv  \n",
    "traindata/maestro-v3.0.0-midi/maestro-v3.0.0/test_indicies.csv   \n",
    "traindata/maestro-v3.0.0-midi/maestro-v3.0.0/validation_indicies.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0440f3",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "implenting utility functions such as the randomizing the data set for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d594a34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### making the random seed \n",
    "np.random.seed(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d296ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomizing the data set index for training purposes\n",
    "def randomizeing(data_set):\n",
    "    ds = np.array(data_set)\n",
    "    np.random.shuffle(ds)\n",
    "    return ds\n",
    "\n",
    "## Select n indices from the givien data set\n",
    "def select_N_randomized_from_set(n, data_set):\n",
    "    nparry = randomizeing(data_set)\n",
    "    return nparry[:n]\n",
    "\n",
    "\n",
    "## ----- ----- ---------- loading function ------------------------------ ##\n",
    "\n",
    "## function loading in the wav function\n",
    "def load_wav_from_index(index):\n",
    "    labels_file_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/maestro-v3.0.0.json\"\n",
    "    with open(labels_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    wav_path = \"traindata/maestro-v3.0.0/maestro-v3.0.0/\" + data['audio_filename'][str(index)]\n",
    "    \n",
    "    return librosa.load(wav_path, sr=None)\n",
    "\n",
    "## function loading in the midi function\n",
    "def load_midi_from_index(index):\n",
    "    labels_file_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/maestro-v3.0.0.json\"\n",
    "    with open(labels_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    midi_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/\" + data['midi_filename'][str(index)]\n",
    "    return pm.PrettyMIDI(midi_path)\n",
    "\n",
    "## ----- ----- -------- Path showing function-------------------------------- ##\n",
    "    \n",
    "## showing the file path audio of the wave\n",
    "def show_wav_path(index):\n",
    "    labels_file_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/maestro-v3.0.0.json\"\n",
    "    with open(labels_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    wav_path = \"traindata/maestro-v3.0.0/maestro-v3.0.0/\" + data['audio_filename'][str(index)]\n",
    "    return wav_path\n",
    "\n",
    "## Showing the file path of the midi file of data[index]\n",
    "def show_midi_path(index):\n",
    "    labels_file_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/maestro-v3.0.0.json\"\n",
    "    with open(labels_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    midi_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/\" + data['midi_filename'][str(index)]\n",
    "    return midi_path\n",
    "\n",
    "## loading in the wav and midi pair\n",
    "def load_wav_midi_pair(index): ## (wav, midi)\n",
    "    return load_wav_from_index(index), load_midi_from_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1db3f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_index_from_csv(\"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/train_indicies.csv\")\n",
    "print(len(train_data))\n",
    "subset_train_data = select_N_randomized_from_set(50, train_data)\n",
    "\n",
    "print(len(subset_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0651ce0a",
   "metadata": {},
   "source": [
    "# AUDIO EXPERIMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abeb97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(wav, sr), midi = load_wav_midi_pair(1025)\n",
    "# midi = load_midi_from_index(1025)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5babb367",
   "metadata": {},
   "outputs": [],
   "source": [
    "ballade1, sr = load_wav_from_index(505)\n",
    "\n",
    "noised_ballade1 = add_gaussian_noise(ballade1, noise_level=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d34459",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ballade1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fdace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(noised_ballade1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf86bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, sr = load_wav_from_index(216)\n",
    "\n",
    "\n",
    "ipd.Audio(data= wav, rate= sr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c3118",
   "metadata": {},
   "source": [
    "### Audio Preprocessing functions\n",
    "\n",
    "#### Pretty_midi note\n",
    "The MIDI object is used from the python package pretty_midi.  \n",
    "Using pretty_midi range from 0 to 127. we can later tranform this into the range of 0 to 87 to match a piano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1cefca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\"\"\" This function extracts all played notes in the midi Object, which it will be futher trained with the aligne ed wave object\n",
    "    input: pm object\n",
    "    It is good for debugging and seeing the midi object\n",
    "\"\"\"\n",
    "def extract_midi_notes(midi):\n",
    "    notes = []\n",
    "    for instrument in midi.instruments:\n",
    "        for note in instrument.notes:\n",
    "            notes.append({\n",
    "                'pitch': note.pitch,\n",
    "                'start': note.start,\n",
    "                'end': note.end,\n",
    "                'velocity': note.velocity\n",
    "            })\n",
    "    #preprocessing the sort\n",
    "    notes.sort(key=(lambda x: x['start']))\n",
    "    return notes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Wrting a peekable Generator for midi object\"\"\"\n",
    "class PeekableGenerator:\n",
    "    def __init__(self, generator):\n",
    "        self._generator = generator\n",
    "        self._next_item = None\n",
    "        self._has_next = False\n",
    "        self._advance()\n",
    "\n",
    "    def _advance(self):\n",
    "        try:\n",
    "            self._next_item = self._generator.__next__()\n",
    "            self._has_next = True\n",
    "        except StopIteration:\n",
    "            self._next_item = None\n",
    "            self._has_next = False\n",
    "\n",
    "    def peek(self):\n",
    "        if not self._has_next:\n",
    "            raise StopIteration(\"No more elements to peek at.\")\n",
    "        return self._next_item\n",
    "\n",
    "    def __next__(self):\n",
    "        if not self._has_next:\n",
    "            raise StopIteration(\"No more elements.\")\n",
    "        current = self._next_item\n",
    "        self._advance()\n",
    "        return current\n",
    "\n",
    "    def has_next(self):\n",
    "        return self._has_next\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield self._next_item\n",
    "        self._advance()\n",
    "\n",
    "    \n",
    "\n",
    "\"\"\"Generator to yield midi note object at the frame during the classification\n",
    "    input: pm object\n",
    "\"\"\"\n",
    "def midi_yielding(midi):\n",
    "    all_midi_obj :list = extract_midi_notes(midi)\n",
    "    ##Processing\n",
    "    for note in all_midi_obj:\n",
    "        yield note\n",
    "\n",
    "\n",
    "\"\"\" Yielding a list of midi notes information where it fits the time frame automatically.\n",
    "    Implemented using overlapping frame structure for the training.\n",
    "    Begin at 0, the frame jumping at the speed of jump_len, the size of the frame is frame_len\n",
    "    This function will yield the frame at the given parameter.\n",
    "\"\"\"\n",
    "def frame_aligning_midi(t: int, jump_len:int, frame_len: int, sr:int, midi):\n",
    "    midi_generator = PeekableGenerator(midi_yielding(midi))\n",
    "    midi_labels = []\n",
    "\n",
    "    jump_time_fraction: float = jump_len * (1/sr)\n",
    "    frame_time_fraction: float = frame_len * (1/sr)\n",
    "    framing = [t*jump_time_fraction, t*jump_time_fraction + frame_time_fraction]\n",
    "    \n",
    "    last_note = 0\n",
    "    while midi_generator.has_next() or last_note > framing[0]:\n",
    "        \n",
    "        while midi_generator.has_next() and midi_generator.peek()['start'] >= framing[0] and midi_generator.peek()['start'] < framing[1]:\n",
    "            try:\n",
    "                midi_labels.append(midi_generator.__next__())\n",
    "            except StopIteration:\n",
    "                print(\"Generator exhausted, no Midi Objectis being added to the label\")\n",
    "                break\n",
    "        \n",
    "        ## Yielding the list of midi notes that are fitted in side the frame\n",
    "        yield midi_labels\n",
    "\n",
    "        midi_labels.sort(key=(lambda x: x['end']))\n",
    "        ##calculating the next frame time step and removing the items from the previous frame\n",
    "        framing [0] += jump_time_fraction\n",
    "        framing [1] += jump_time_fraction\n",
    "        while len(midi_labels) > 0 and midi_labels[0]['end'] < framing[0]:\n",
    "            midi_labels.pop(0)\n",
    "        \n",
    "        if len(midi_labels) > 0 and midi_labels[0]['end'] > framing[0]:\n",
    "            last_note = midi_labels[0]['end']\n",
    "\n",
    "    \n",
    "        \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\" This function returns the aligned frame at the wav data,\"\"\"\n",
    "def frame_aligning_wav(t: int, jump_len: int, frame_len: int, wav):\n",
    "    begin = t * jump_len\n",
    "    return wav[begin: begin + frame_len]\n",
    "\n",
    "\n",
    "\"\"\" This function returns the amount seconds of audio data from the wav, began on t, while using  \"\"\"\n",
    "\n",
    "def audio_segment_of(t: int, wav, seconds: float, sr: int, jump_len: int = 512, frame_len: int = 2048, ):\n",
    "    size = int(seconds*sr)\n",
    "    begin = t*jump_len\n",
    "    \n",
    "    return wav[begin: begin + size]\n",
    "\n",
    "def audio_segment_between(begin, end, wav, sr):\n",
    "    return wav[int(begin*sr): int(end*sr)]\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260c0e13",
   "metadata": {},
   "source": [
    "### Constructing Mel Spectrogram From wav frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8191371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mel_spectrogram(audio, sr, n_mels=88, hop_length=512, n_fft=4096): ##_fft is the frame length \n",
    "    \"\"\"\n",
    "    Extract a mel-spectrogram from raw audio.\n",
    "    \"\"\"\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio, sr=sr, n_mels=n_mels, hop_length=hop_length, n_fft=n_fft\n",
    "    )\n",
    "    # Convert to log scale\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return mel_spec_db\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b5e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example audio input\n",
    "frame_time = 2048 /sr\n",
    "num_frame = 22\n",
    "audio1 = audio_segment_between(0,2,ballade1, sr)\n",
    "quick_sample = audio_segment_between(0, num_frame*frame_time, ballade1, sr)\n",
    "mel_spectrogram = extract_mel_spectrogram(audio1, sr, n_mels =188)\n",
    "print(mel_spectrogram.shape)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "librosa.display.specshow(mel_spectrogram, \n",
    "                         sr=sr, \n",
    "                         x_axis=\"linear\")\n",
    "plt.colorbar(format=\"%+2.f\")\n",
    "plt.show()\n",
    "ipd.Audio(data= audio1, rate= sr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d73318",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e9eb5b8",
   "metadata": {},
   "source": [
    "### Calculating the added noise level to the wav function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea68b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_noise_power(audio, snr_dB):\n",
    "    \"\"\"\n",
    "    Compute the noise power needed for a given SNR in dB.\n",
    "    :param audio: NumPy array of the audio signal.\n",
    "    :param snr_dB: Desired Signal-to-Noise Ratio in dB.\n",
    "    :return: Noise power.\n",
    "    \"\"\"\n",
    "    # Calculate signal power (mean squared amplitude)\n",
    "    signal_power = np.mean(audio ** 2)\n",
    "    \n",
    "    # Convert SNR from dB to linear scale\n",
    "    snr_linear = 10 ** (snr_dB / 10)\n",
    "    \n",
    "    # Calculate noise power\n",
    "    noise_power = signal_power / snr_linear\n",
    "    return noise_power\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c50b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(audio, noise_level=0.0006):\n",
    "    \n",
    "    noise = np.random.normal(0, noise_level, audio.shape)\n",
    "    return audio + noise\n",
    "\n",
    "def framelining (time, jump, sr):\n",
    "    \n",
    "    frametime = jump/sr\n",
    "    print(22*frametime)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c59f752",
   "metadata": {},
   "source": [
    "### Model for musheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a81def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PianoNoteModel(nn.Module):\n",
    "    def __init__(self, num_mel_bins=88, mel_temporal_length=89, num_frame_output=22, output_size=(88, 3)):\n",
    "        \"\"\"The default parameter is approximated for 1 seconds of audio data, regarding to the temporal_length\"\"\"\n",
    "        super(PianoNoteModel, self).__init__()\n",
    "\n",
    "        self.num_mel_bins = num_mel_bins\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        # Compute flattened size based on input dimensions after pooling\n",
    "        # Assuming input shape is (batch_size, 1, num_mel_bins, num_frames)\n",
    "        pooled_mel_bins = num_mel_bins // 2  # Adjust based on pooling\n",
    "        pooled_temporal_length = mel_temporal_length // 2     # Adjust based on pooling\n",
    "\n",
    "        \n",
    "        flattened_size = pooled_mel_bins * pooled_temporal_length * 64  # Based on conv2 output channels\n",
    "        \n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(flattened_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_frame_output * output_size[0] * output_size[1])  # Predict for each frame\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, 1, num_mel_bins, num_frames)\n",
    "        \n",
    "        # Convolutional layers\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "\n",
    "       \n",
    "\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten except batch dimension\n",
    "       \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "       \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Reshape to output dimensions: (batch_size, num_frames, 88, 3)\n",
    "        x = x.view(x.size(0), -1, 88, 3)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "# Custom loss function, for mutipurpose loss function in the output layer\n",
    "class MultiTaskLoss(nn.Module):\n",
    "    def __init__(self, classification_weight=1.0, regression_weight=1.0):\n",
    "        super(MultiTaskLoss, self).__init__()\n",
    "        self.classification_loss = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        self.regression_loss = nn.MSELoss()\n",
    "        self.classification_weight = classification_weight\n",
    "        self.regression_weight = regression_weight\n",
    "\n",
    "    def forward(self, classification_output, classification_target, \n",
    "                regression_output, regression_target, ratio):\n",
    "        # Compute classification loss\n",
    "        loss_per_element  = self.classification_loss(classification_output, classification_target)\n",
    "\n",
    "        pressed_key_mask = classification_target == 1\n",
    "        unpressed_key_mask = classification_target == 0\n",
    "\n",
    "        pressed_key_label = classification_target[pressed_key_mask]\n",
    "        unpressed_key_label = classification_target[unpressed_key_mask]\n",
    "\n",
    "\n",
    "        pressed_keynumber = pressed_key_label.numel()\n",
    "        unpressed_keynumber = unpressed_key_label.numel()\n",
    "\n",
    "\n",
    "        # Compute the scoring of pressedkey and unpressed key with the feedback\n",
    "        w_pressed = ((unpressed_keynumber/pressed_keynumber)**(ratio/2)) #* feed_back[0]\n",
    "        w_unpressed= ((pressed_keynumber/unpressed_keynumber)**(ratio/2)) #* feed_back[1]\n",
    "        #total =  pressed_keynumber + unpressed_keynumber\n",
    "        #w_pressed = unpressed_keynumber /total\n",
    "        #w_unpressed = pressed_keynumber / total\n",
    "\n",
    "        weight_matrix = torch.full_like(classification_target, w_unpressed)\n",
    "        weight_matrix[pressed_key_mask] = w_pressed\n",
    "        \n",
    "        weighted_class_loss_mat = loss_per_element * weight_matrix\n",
    "        \n",
    "        weighted_class_loss = weighted_class_loss_mat.sum()\n",
    "        \n",
    "        \n",
    "        \"\"\"# Computing the current accuracy of pressed and non pressed keys\n",
    "        predicted_class = (classification_output > 0.5).float()\n",
    "        correct_pressed = (predicted_class[pressed_key_mask] == classification_target[pressed_key_mask]).sum().item()\n",
    "        correct_unpressed = (predicted_class[unpressed_key_mask] == classification_target[unpressed_key_mask]).sum().item()\n",
    "        total_pressed = pressed_key_mask.sum().item()\n",
    "        total_unpressed = unpressed_key_mask.sum().item()\n",
    "\n",
    "\n",
    "\n",
    "        # update the feedback based on the accuracy\n",
    "        epsilon = 1e-6  # Small constant to avoid instability\n",
    "        correct_pressed_ratio = max(correct_pressed / total_pressed, epsilon)\n",
    "        correct_unpressed_ratio = max(correct_unpressed / total_unpressed, epsilon)\n",
    "\n",
    "        feed_back[0] = 1 / correct_pressed_ratio\n",
    "        feed_back[1] = 1 / correct_unpressed_ratio\"\"\"\n",
    "\n",
    "        # Compute regression loss\n",
    "        reg_loss = self.regression_loss(regression_output, regression_target)\n",
    "        \n",
    "        # Combine with weights\n",
    "        total_loss = self.classification_weight * weighted_class_loss + self.regression_weight * reg_loss\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def set_classification_balancer(self, weight):\n",
    "        self.classification_loss.weight = weight\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04663d46",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78ededb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm\n",
    "\n",
    "## MODEL Saver\n",
    "def save_model(model, optimizer, id, path = \"models/\"):\n",
    "    \"\"\" Saving the model after each training/testing before each training progress\"\"\"\n",
    "    actualPath = path + f\"{id}_piano_model.pth\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, actualPath)\n",
    "    print(f\"Model saved to {actualPath}\")\n",
    "\n",
    "## MODEL loader\n",
    "def load_model(model, optimizer, id, path = \"models/\"):\n",
    "    actualPath = path + f\"{id}_piano_model.pth\"\n",
    "    checkpoint = torch.load(actualPath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "\"\"\"This function generates the matrix label to aligne the mal_spectral gram\n",
    "each matrix should consired num_frame,\n",
    "poping fram_jumping after the matrix is being yield\n",
    "\n",
    "jumping time should be\n",
    "frame_jumping * (frame_len /sr)\n",
    "\"\"\"\n",
    "def label_generator(num_frame, frame_jumping, jump_len:int, frame_len: int, sr:int, midi):\n",
    "    midi_frame_gen = PeekableGenerator(frame_aligning_midi(0, frame_len, frame_len, sr, midi)) ## Note, while using the architechture of mel_spectrogram, the we don't need to consider the jump offset in the midi side\n",
    "    label = []\n",
    "    counter = 0\n",
    "    \n",
    "    \n",
    "    concur_time = 0\n",
    "    concur_time_end = 0\n",
    "    frame_time = frame_len /sr\n",
    "\n",
    "    \n",
    "    while midi_frame_gen.has_next():\n",
    "        if (len(label) < num_frame):\n",
    "            label.append(list(midi_frame_gen.__next__()))\n",
    "            concur_time_end += frame_time\n",
    "            \n",
    "        else:\n",
    "            counter+=1\n",
    "            yield label\n",
    "            for i in range(int(frame_jumping)):\n",
    "                label.pop(0)\n",
    "                concur_time += frame_time\n",
    "                \n",
    "\n",
    "\n",
    "\"\"\"This function tries to mimic the decayed velocity miniking the sound at which a piano has been decayed\"\"\"\n",
    "def velocity_decay_sustain (velocity, onset, at_time):\n",
    "    if (at_time - onset < 0.2):\n",
    "        return velocity\n",
    "    else:\n",
    "        return math.exp((onset - at_time) * 0.6) * velocity\n",
    "\n",
    "\n",
    "\"\"\"formating the label of list, in to a matrix of 88 * 3 matrix.\n",
    "    each row represent a strikable key, \n",
    "    column 1 (being stricked) : 0, 1 (classification purpose)\n",
    "    column 2 (onset timer) : the set of positive interger that is less than onset. (Regression purpose)\n",
    "    column 3 (velocity of which is being stricked) : the set of positive integer that is less than onset. (Regression purpose) \n",
    "        note for the velocity of the piano key will be approximately alingned with a decay parameter\"\"\"\n",
    "def label_formater(label, frameonset):\n",
    "    ret_label = np.zeros((len(label), 88,3))\n",
    "\n",
    "    for i in range(len(label)) :\n",
    "        for key_obj in label[i]:\n",
    "            pitch = key_obj['pitch'] - 20\n",
    "            ret_label[i][pitch][0] = 1\n",
    "            ret_label[i][pitch][1] = key_obj['start'] - frameonset if key_obj['start'] > frameonset else 0\n",
    "            ret_label[i][pitch][2] = velocity_decay_sustain(key_obj['velocity'],  key_obj['start'], frameonset) \n",
    "\n",
    "    return ret_label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"This function assure the input for the training will retain the dimension in the case of track is ending.\"\"\"\n",
    "def construct_input(spectrogram, x, y):\n",
    "    if spectrogram.shape == (x,y):\n",
    "        return spectrogram\n",
    "    else: \n",
    "        ret = np.zeros((x,y))\n",
    "        \n",
    "       \n",
    "        for i in range(spectrogram.shape[0]):\n",
    "            ret[i][:spectrogram.shape[1]] = spectrogram[i][:spectrogram.shape[1]]\n",
    "        return ret\n",
    "\n",
    "def train_segment(wav, sr, length=1, hop=0.5):\n",
    "    ret = []\n",
    "    max = len(wav)/sr\n",
    "    begin = 0\n",
    "    while (begin < max):\n",
    "        adding = length if begin + length < max else max - begin\n",
    "        ret.append((begin, begin + adding))\n",
    "        begin += hop\n",
    "\n",
    "    return ret\n",
    "    \n",
    "\n",
    "def label_buffer(num_frame, label):\n",
    "    if len(label) < num_frame:\n",
    "        for i in range(num_frame- len(label)):\n",
    "            label.append([])\n",
    "    return label\n",
    "\n",
    "\n",
    "\"\"\" This function will generate all the mel_spectrogram and label pair for each audio segment of the song at index\"\"\"\n",
    "def generate_data_label(index, noised=True, num_frame=22, segment_jump = 0.5, frame_length = 2048, hop_length = 512):\n",
    "    # fetching infomation\n",
    "    (wav, sr), midi = load_wav_midi_pair(index)\n",
    "    \n",
    "    if noised:\n",
    "        wav = add_gaussian_noise(wav)\n",
    "\n",
    "    frame_time =  frame_length/sr\n",
    "    quick_sample = audio_segment_between(0, num_frame*frame_time, wav, sr)\n",
    "    x, y = extract_mel_spectrogram(quick_sample, sr).shape\n",
    "    segments = train_segment(wav, sr, length = num_frame * frame_time, hop= num_frame * frame_time/2)\n",
    "    \n",
    "    label_gen = PeekableGenerator(label_generator(num_frame, num_frame*segment_jump, hop_length, frame_length, sr, midi))\n",
    "    \n",
    "    input_datas = []\n",
    "    labels = []\n",
    "    for beg, end in segments:\n",
    "        \n",
    "        if (label_gen.has_next() is False) :\n",
    "            break\n",
    "\n",
    "        audio = audio_segment_between(beg, end, wav, sr)\n",
    "        mel_spectrogram = extract_mel_spectrogram(audio, sr, hop_length= hop_length, n_fft=frame_length)\n",
    "        input_data = construct_input(mel_spectrogram, x,y)\n",
    "        input_data = np.array(input_data)\n",
    "        input_data = np.expand_dims(input_data, axis=0)\n",
    "\n",
    "        label = label_buffer(num_frame, label_gen.__next__())\n",
    "        label = label_formater(label, beg)\n",
    "        label = np.array(label)\n",
    "\n",
    "        input_datas.append(input_data)\n",
    "        labels.append(label)\n",
    "        \n",
    "    return np.array(input_datas), np.array(labels)\n",
    "   \n",
    "def generate_data(index, num_frame=22, segment_jump = 0.5, frame_length = 2048, hop_length = 512):\n",
    "    \"\"\"This function generates the audio mel_spectrogram for the song at {index} with the defaulted training parameter\n",
    "        this function is generally used for validation process.\n",
    "    \"\"\"\n",
    "    # fetching infomation\n",
    "    (wav, sr), midi = load_wav_midi_pair(index)\n",
    "    wav, sr = load_wav_from_index(index)\n",
    "    \n",
    "    frame_time =  frame_length/sr\n",
    "    quick_sample = audio_segment_between(0, num_frame*frame_time, wav, sr)\n",
    "    x, y = extract_mel_spectrogram(quick_sample, sr).shape\n",
    "    segments = train_segment(wav, sr, length = num_frame * frame_time, hop= num_frame * frame_time/2)\n",
    "    input_datas = []\n",
    " \n",
    "    for beg, end in segments:\n",
    "        audio = audio_segment_between(beg, end, wav, sr)\n",
    "        mel_spectrogram = extract_mel_spectrogram(audio, sr, hop_length= hop_length, n_fft=frame_length)\n",
    "        input_data = construct_input(mel_spectrogram, x,y)\n",
    "        input_data = np.array(input_data)\n",
    "        input_data = np.expand_dims(input_data, axis=0)   \n",
    "        input_datas.append(input_data)\n",
    "        \n",
    "    return np.array(input_datas)\n",
    "\n",
    "\n",
    "\n",
    "def one_pass_song_train(id, index, ratio,  noised=True, num_frame=22, segment_jump = 0.5, frame_length = 2048\n",
    "                        , hop_length = 512, batch_size=16 ):\n",
    "    #load model\n",
    "    \"\"\"The training is based on how many frame should be trained at a time,\n",
    "        the default setting is suited for the expriment set up above,\n",
    "        num_frame is tried to aligned it to approx 1 second of the sample\n",
    "        segment_jumping would be trying to get 50% of the sample audio\n",
    "        The dimension of the input is:\n",
    "        batchsize * channel * \n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = PianoNoteModel()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    #Letting the classification utility be more potent than the regression utility.\n",
    "    criterion = MultiTaskLoss(classification_weight=1.7, regression_weight=0.5)\n",
    " \n",
    "    \n",
    "    model, optimizer = load_model(model, optimizer, id)\n",
    "\n",
    "    # generate all data\n",
    "    datas, labels = generate_data_label(index, noised=noised, num_frame=num_frame, segment_jump=segment_jump, frame_length=frame_length, hop_length=hop_length)\n",
    "    datas = torch.tensor(datas, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    dataset = TensorDataset(datas, labels)\n",
    "    # Let model be in training mode\n",
    "    model.train()\n",
    "    dataloader = DataLoader(dataset, batch_size = batch_size, shuffle =True)\n",
    "    progress_bar = tqdm.tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    # Initialize the feedback value before training to \n",
    "    \n",
    "\n",
    "    lossval = 0.0\n",
    "    for _, (batch_inputs, batch_labels) in progress_bar:\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(batch_inputs)\n",
    "\n",
    "        #  classification training\n",
    "        classification_output = outputs[..., 0:1]\n",
    "        classification_target = batch_labels[..., 0:1]\n",
    "        \n",
    "        #  regression training\n",
    "        regression_output = outputs[..., 1:3]\n",
    "        regression_target = batch_labels[...,1:3]\n",
    "        loss = criterion.forward(classification_output, classification_target, regression_output, regression_target, ratio)\n",
    "    \n",
    "        \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        lossval+= loss.item()\n",
    "\n",
    "        \n",
    "    #save model\n",
    "    save_model(model, optimizer, id)\n",
    "    return lossval\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_N_song_on_epoch (id, epoch, N, repetition = 2, noised=True):\n",
    "    \n",
    "\n",
    "    tobe_done = load_progress_index_from_csv(epoch)\n",
    "\n",
    "    ## indices to be trained for the current run\n",
    "    parse_in = [tobe_done.pop() for i in range(N)]\n",
    "\n",
    "    feed_back_tensor = feed_back = torch.tensor([1.0,1.0])\n",
    "    \n",
    "    ## Passing in each index to train on one_pass_song_train\n",
    "    for index in parse_in:\n",
    "        one_pass_song_train(id, index)\n",
    "\n",
    "    print(\"Message from train_N_song_on_epoch\") \n",
    "    if (len(tobe_done) == 0):\n",
    "        print(f\"Epoch {epoch} training complete\")\n",
    "    else :\n",
    "        \n",
    "        print(f\"Epoch {epoch} have trained {N} songs, left over songs for current epoch will be saved. Need to train {len(tobe_done)}.\")\n",
    "        save_progress_index_to_csv(tobe_done, epoch)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c448858",
   "metadata": {},
   "source": [
    "### Validation functions\n",
    "\n",
    "Use the function below to test the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "56772593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validation_accuracy_check(id, index, noised=True, num_frame=22, segment_jump = 0.5, frame_length = 2048, hop_length = 512, batch_size=16):\n",
    "    \"\"\"This function returns the accuracy of the model predicting the song at index\n",
    "        arguments:\n",
    "            id: trained model id\n",
    "            index: index of the song.\n",
    "    \"\"\"\n",
    "    model = PianoNoteModel()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    #Letting the classification utility be more potent than the regression utility.\n",
    "    criterion = MultiTaskLoss(classification_weight=1.5, regression_weight=0.7)\n",
    " \n",
    "    \n",
    "    model, optimizer = load_model(model, optimizer, id)\n",
    "   \n",
    "\n",
    "    datas, labels = generate_data_label(index, num_frame=num_frame, segment_jump=segment_jump, frame_length=frame_length, hop_length=hop_length)\n",
    "    \n",
    "\n",
    "    datas = torch.tensor(datas, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    dataset = TensorDataset(datas, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    classification_correct = 0\n",
    "    classification_true_correct = 0\n",
    "    regression_error = 0\n",
    "\n",
    "    true_classified = 0\n",
    "    total_classifed = 0\n",
    "    num_classified = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for val_inputs, val_labels in dataloader:\n",
    "            outputs = model(val_inputs)\n",
    "\n",
    "            #Output separation\n",
    "            classification_outputs = outputs[:,:,:, 0:1].squeeze(-1)\n",
    "            regression_outputs = outputs[:, :, :, 1:3]\n",
    "\n",
    "            #Label separation\n",
    "            classification_labels = val_labels[:, :, :, 0:1].squeeze(-1)\n",
    "            regression_labels = val_labels[:, :, :, 1:3]\n",
    "            \n",
    "            #Calculate the classification accuracy of a frame\n",
    "            predicted_classes = (classification_outputs > 0.5).float()\n",
    "            classification_correct += (predicted_classes == classification_labels).sum().item() / model.num_mel_bins #this should be the accuracy of a frame\n",
    "\n",
    "\n",
    "            #Calculate the classification accuracy of pressed key\n",
    "            pressed_key_mask = classification_labels == 1\n",
    "            pressed_key_label = classification_labels[pressed_key_mask]\n",
    "            pressed_key_prediction = predicted_classes[pressed_key_mask]\n",
    "            true_classified += pressed_key_label.numel()\n",
    "            total_classifed += classification_labels.numel()\n",
    "            classification_true_correct += (pressed_key_label == pressed_key_prediction).sum().item()\n",
    "            \n",
    "\n",
    "            regression_error += ((regression_outputs - regression_labels) ** 2).mean().item()\n",
    "            \n",
    "            num_classified += val_labels.shape[0] * val_labels.shape[1] # counting number of frames have been classified i.e. batch_number * number of frame per segment\n",
    "            \n",
    "\n",
    "    if true_classified > 0 :\n",
    "        true_accuracy = classification_true_correct/true_classified\n",
    "    else:\n",
    "        true_accuracy = 0\n",
    "    print(f\"Classified {num_classified} frames of audio, classification arcuracy {classification_correct/ num_classified}, average regression error is {regression_error/ num_classified}\")\n",
    "    print(f\"The true accuracy of classifying pressed_key are {true_accuracy}\")\n",
    "    print(f\"Ratio of pressed is in data is {true_classified/total_classifed}\")\n",
    "    return num_classified, classification_correct, regression_error\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def model_output(id, index):\n",
    "    \"\"\" This function returns the audio classification from the model {id}\n",
    "        arguments : id (model id)\n",
    "                    index (song index)\n",
    "            return: classification result\n",
    "    \"\"\"\n",
    "    model = PianoNoteModel()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    model, _ = load_model(model, optimizer, id)\n",
    "    audio_data = generate_data(index)\n",
    "\n",
    "\n",
    "    datas = torch.tensor(audio_data, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    dataloader = DataLoader(audio_data, batch_size = 16, shuffle =True)\n",
    "    \n",
    "\n",
    "\n",
    "    dataset = TensorDataset(datas, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size = batch_size, shuffle =True)\n",
    "\n",
    "    \n",
    "    progress_bar = tqdm.tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    for batch_inputs, batch_labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(dataloader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def midi_reconstruction(outputed_data):\n",
    "    \"\"\"Reconstruct the midi object from the outputed_data.\n",
    "        Arguments:\n",
    "            outputed_data: A list of outputed matrixies[batch_size * frame_number * 88 * 3]\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d5066c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b42ac20",
   "metadata": {},
   "source": [
    "### Index Processing\n",
    "\n",
    "Using the saved indices we have processed before  \n",
    "Here are the file paths  \n",
    "for training datas :traindata/maestro-v3.0.0-midi/maestro-v3.0.0/train_indicies.csv  \n",
    "for test datas :traindata/maestro-v3.0.0-midi/maestro-v3.0.0/test_indicies.csv   \n",
    "for validataion datas :traindata/maestro-v3.0.0-midi/maestro-v3.0.0/validation_indicies.csv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64563c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only run this once\n",
    "train_indices = load_index_from_csv(\"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/train_indicies.csv\")\n",
    "validation_indices = load_index_from_csv(\"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/validation_indicies.csv\")\n",
    "test_indices = load_index_from_csv(\"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/test_indicies.csv\")\n",
    "\n",
    "def initiate_train_epoch(epoch_Number, dataset):\n",
    "    train_indices = load_index_from_csv(dataset)\n",
    "    train_indices = randomizeing(train_indices)\n",
    "    save_progress_index_to_csv(train_indices, epoch_Number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc9808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the training epoch. RUN ONCE BEFORE TRAINING\n",
    "#initiate_train_epoch(0, \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/train_indicies.csv\")\n",
    "\n",
    "# Initiate the training model. RUN ONCE BEFORE TRAINING\n",
    "model = PianoNoteModel(output_size=(88, 3))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "save_model(model, optimizer, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f318a2c7",
   "metadata": {},
   "source": [
    "## Trainign prototype\n",
    "Experimenting training on one single sample to see if there are any progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa7cc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicies = [686, 1272, 216, 1024, 962, 505]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5aa3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in indicies:\n",
    "    final_loss = 0\n",
    "    for i in range(3): \n",
    "        train_lost_val = one_pass_song_train(\"prototype\", index)\n",
    "        print(train_lost_val)\n",
    "        final_loss = train_lost_val\n",
    "    print(f\"final loss val for {index} is {final_loss}\")\n",
    "    \n",
    "a, b, c = validation_accuracy_check(\"prototype\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43768c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PianoNoteModel(output_size=(88, 3))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "save_model(model, optimizer, \"prototype3\")\n",
    "\n",
    "for index in indicies:\n",
    "    final_loss = 0\n",
    "    for i in range(3): \n",
    "        train_lost_val = one_pass_song_train(\"prototype3\", index)\n",
    "        print(train_lost_val)\n",
    "        final_loss = train_lost_val\n",
    "    print(f\"final loss val for {index} is {final_loss}\")\n",
    "\n",
    "a, b, c = validation_accuracy_check(\"prototype3\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e879d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PianoNoteModel(output_size=(88, 3))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "save_model(model, optimizer, \"prototype0\")\n",
    "\n",
    "for index in indicies:\n",
    "    final_loss = 0\n",
    "    for i in range(3): \n",
    "        train_lost_val = one_pass_song_train(\"prototype0\", index)\n",
    "        print(train_lost_val)\n",
    "        final_loss = train_lost_val\n",
    "    print(f\"final loss val for {index} is {final_loss}\")\n",
    "\n",
    "a, b, c = validation_accuracy_check(\"prototype0\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f463d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = validation_accuracy_check(\"prototype0\", 9 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9df08ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype01_piano_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youxu\\AppData\\Local\\Temp\\ipykernel_9368\\2953729452.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(actualPath)\n",
      "100%|██████████| 15/15 [00:06<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype01_piano_model.pth\n",
      "56083264517.15625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype01_piano_model.pth\n",
      "3209031.17578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype01_piano_model.pth\n",
      "1763497.34765625\n",
      "final loss val for 686 is 1763497.34765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype01_piano_model.pth\n",
      "246772163600.6172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype01_piano_model.pth\n",
      "492003723264.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype01_piano_model.pth\n",
      "486493724672.0\n",
      "final loss val for 1272 is 486493724672.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:14<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype01_piano_model.pth\n",
      "1172117618688.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:13<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype01_piano_model.pth\n",
      "1144090434560.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:13<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype01_piano_model.pth\n",
      "1120568452096.0\n",
      "final loss val for 216 is 1120568452096.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype01_piano_model.pth\n",
      "1303839901696.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:15<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype01_piano_model.pth\n",
      "1269778300928.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype01_piano_model.pth\n",
      "1238544189440.0\n",
      "final loss val for 1024 is 1238544189440.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype01_piano_model.pth\n",
      "1292461384704.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype01_piano_model.pth\n",
      "1243894062080.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype01_piano_model.pth\n",
      "1203891028992.0\n",
      "final loss val for 962 is 1203891028992.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:26<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype01_piano_model.pth\n",
      "1933284633600.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:27<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype01_piano_model.pth\n",
      "706383815805.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:27<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype01_piano_model.pth\n",
      "140088208456.75\n",
      "final loss val for 505 is 140088208456.75\n",
      "Classified 21758 frames of audio, classification arcuracy 0.9654181534064799, average regression error is 0.19379167871688238\n",
      "The true accuracy of classifying pressed_key are 0.0033094148212297414\n"
     ]
    }
   ],
   "source": [
    "model = PianoNoteModel(output_size=(88, 3))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "save_model(model, optimizer, \"prototype01\")\n",
    "\n",
    "for index in indicies:\n",
    "    final_loss = 0\n",
    "    for i in range(3): \n",
    "        train_lost_val = one_pass_song_train(\"prototype01\", index)\n",
    "        print(train_lost_val)\n",
    "        final_loss = train_lost_val\n",
    "    print(f\"final loss val for {index} is {final_loss}\")\n",
    "\n",
    "a, b, c = validation_accuracy_check(\"prototype01\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cef3a65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_piano_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youxu\\AppData\\Local\\Temp\\ipykernel_9368\\2953729452.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(actualPath)\n",
      "100%|██████████| 15/15 [00:06<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_piano_model.pth\n",
      "6799158.517578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_piano_model.pth\n",
      "390816.9296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_piano_model.pth\n",
      "164010.56420898438\n",
      "final loss val for 686 is 164010.56420898438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:05<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_piano_model.pth\n",
      "128903.92578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:05<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_piano_model.pth\n",
      "63003.138671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:05<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_piano_model.pth\n",
      "43115.482666015625\n",
      "final loss val for 1272 is 43115.482666015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:13<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_piano_model.pth\n",
      "555276.9255371094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:13<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_piano_model.pth\n",
      "164137.896484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:14<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_piano_model.pth\n",
      "136453.73083496094\n",
      "final loss val for 216 is 136453.73083496094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_piano_model.pth\n",
      "247164.97387695312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_piano_model.pth\n",
      "195423.67944335938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_piano_model.pth\n",
      "184074.65502929688\n",
      "final loss val for 1024 is 184074.65502929688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:16<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_piano_model.pth\n",
      "260581.00170898438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:16<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_piano_model.pth\n",
      "217940.33081054688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:16<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_piano_model.pth\n",
      "213302.40209960938\n",
      "final loss val for 962 is 213302.40209960938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:26<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_piano_model.pth\n",
      "406124.0244140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:26<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_piano_model.pth\n",
      "321300.5856933594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:26<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_piano_model.pth\n",
      "311983.3181152344\n",
      "final loss val for 505 is 311983.3181152344\n",
      "Classified 21758 frames of audio, classification arcuracy 0.23040428181066114, average regression error is 0.18628961889388115\n",
      "The true accuracy of classifying pressed_key are 0.9920821477174316\n"
     ]
    }
   ],
   "source": [
    "model = PianoNoteModel(output_size=(88, 3))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "save_model(model, optimizer, \"prototype_nofeedback\")\n",
    "\n",
    "for index in indicies:\n",
    "    final_loss = 0\n",
    "    for i in range(3): \n",
    "        train_lost_val = one_pass_song_train(\"prototype_nofeedback\", index)\n",
    "        print(train_lost_val)\n",
    "        final_loss = train_lost_val\n",
    "    print(f\"final loss val for {index} is {final_loss}\")\n",
    "\n",
    "a, b, c = validation_accuracy_check(\"prototype_nofeedback\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d591de3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_batchrep_piano_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youxu\\AppData\\Local\\Temp\\ipykernel_9368\\1372277917.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(actualPath)\n",
      "100%|██████████| 15/15 [00:12<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_batchrep_piano_model.pth\n",
      "1970549.6208496094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:12<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_batchrep_piano_model.pth\n",
      "149679.1251220703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:12<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_batchrep_piano_model.pth\n",
      "82272.27325439453\n",
      "final loss val for 686 is 82272.27325439453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:12<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_batchrep_piano_model.pth\n",
      "89630.12145996094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:11<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_batchrep_piano_model.pth\n",
      "46985.38513183594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:12<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_batchrep_piano_model.pth\n",
      "37164.93347167969\n",
      "final loss val for 1272 is 37164.93347167969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:28<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_batchrep_piano_model.pth\n",
      "393909.2137451172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:27<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_batchrep_piano_model.pth\n",
      "138123.25390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:29<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_batchrep_piano_model.pth\n",
      "131131.49645996094\n",
      "final loss val for 216 is 131131.49645996094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_batchrep_piano_model.pth\n",
      "225421.01049804688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:31<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_batchrep_piano_model.pth\n",
      "189239.9132080078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:32<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_batchrep_piano_model.pth\n",
      "179403.76501464844\n",
      "final loss val for 1024 is 179403.76501464844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:33<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_batchrep_piano_model.pth\n",
      "240384.32836914062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:33<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_batchrep_piano_model.pth\n",
      "212423.6015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:33<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_batchrep_piano_model.pth\n",
      "205143.53186035156\n",
      "final loss val for 962 is 205143.53186035156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:52<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_batchrep_piano_model.pth\n",
      "372390.2529296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:53<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_batchrep_piano_model.pth\n",
      "325973.0985107422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:52<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_batchrep_piano_model.pth\n",
      "323627.29553222656\n",
      "final loss val for 505 is 323627.29553222656\n",
      "Classified 21758 frames of audio, classification arcuracy 0.20748585682173334, average regression error is 0.1865961725017759\n",
      "The true accuracy of classifying pressed_key are 0.9963967586292218\n"
     ]
    }
   ],
   "source": [
    "model = PianoNoteModel(output_size=(88, 3))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "save_model(model, optimizer, \"prototype_nofeedback_batchrep\")\n",
    "\n",
    "for index in indicies:\n",
    "    final_loss = 0\n",
    "    for _ in range(3):\n",
    "        train_lost_val = one_pass_song_train(\"prototype_nofeedback_batchrep\", index)\n",
    "        print(train_lost_val)\n",
    "        final_loss = train_lost_val\n",
    "    print(f\"final loss val for {index} is {final_loss}\")\n",
    "\n",
    "a, b, c = validation_accuracy_check(\"prototype_nofeedback_batchrep\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6d8c5622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback1_piano_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youxu\\AppData\\Local\\Temp\\ipykernel_9368\\3530722454.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(actualPath)\n",
      "100%|██████████| 15/15 [00:06<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback1_piano_model.pth\n",
      "2355270.1752929688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback1_piano_model.pth\n",
      "193579.07739257812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback1_piano_model.pth\n",
      "120922.5654296875\n",
      "final loss val for 686 is 120922.5654296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:05<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback1_piano_model.pth\n",
      "69639.71997070312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback1_piano_model.pth\n",
      "41562.43212890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback1_piano_model.pth\n",
      "32892.855712890625\n",
      "final loss val for 1272 is 32892.855712890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:13<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback1_piano_model.pth\n",
      "393200.7448730469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:14<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback1_piano_model.pth\n",
      "122372.3408203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:13<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback1_piano_model.pth\n",
      "104303.11596679688\n",
      "final loss val for 216 is 104303.11596679688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback1_piano_model.pth\n",
      "180973.32788085938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback1_piano_model.pth\n",
      "153291.513671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback1_piano_model.pth\n",
      "146705.4033203125\n",
      "final loss val for 1024 is 146705.4033203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback1_piano_model.pth\n",
      "197546.08837890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback1_piano_model.pth\n",
      "162928.06408691406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback1_piano_model.pth\n",
      "146572.0242919922\n",
      "final loss val for 962 is 146572.0242919922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:26<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback1_piano_model.pth\n",
      "294349.59765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:26<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback1_piano_model.pth\n",
      "245903.02111816406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:26<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback1_piano_model.pth\n",
      "229524.08837890625\n",
      "final loss val for 505 is 229524.08837890625\n",
      "Classified 21758 frames of audio, classification arcuracy 0.3527537415704986, average regression error is 0.18545492647193637\n",
      "The true accuracy of classifying pressed_key are 0.9719472967957442\n",
      "Ratio of pressed is in data is 0.033772321988150646\n"
     ]
    }
   ],
   "source": [
    "model = PianoNoteModel(output_size=(88, 3))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "save_model(model, optimizer, \"prototype_nofeedback1\")\n",
    "\n",
    "for index in indicies:\n",
    "    final_loss = 0\n",
    "    for i in range(3): \n",
    "        train_lost_val = one_pass_song_train(\"prototype_nofeedback1\", index)\n",
    "        print(train_lost_val)\n",
    "        final_loss = train_lost_val\n",
    "    print(f\"final loss val for {index} is {final_loss}\")\n",
    "\n",
    "a, b, c = validation_accuracy_check(\"prototype_nofeedback1\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "556e5f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youxu\\AppData\\Local\\Temp\\ipykernel_9368\\3530722454.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(actualPath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified 21758 frames of audio, classification arcuracy 0.20990085151543, average regression error is 0.18621120444991285\n",
      "The true accuracy of classifying pressed_key are 0.9962575776320673\n",
      "Ratio of pressed is in data is 0.033772321988150646\n"
     ]
    }
   ],
   "source": [
    "a, b, c = validation_accuracy_check(\"prototype_nofeedback1\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d60da6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.75_piano_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youxu\\AppData\\Local\\Temp\\ipykernel_9368\\3530722454.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(actualPath)\n",
      "100%|██████████| 15/15 [00:06<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.75_piano_model.pth\n",
      "1832804.3347167969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.75_piano_model.pth\n",
      "183632.38256835938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.75_piano_model.pth\n",
      "100601.05004882812\n",
      "final loss val for 686 is 100601.05004882812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.75_piano_model.pth\n",
      "71936.14184570312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:05<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.75_piano_model.pth\n",
      "42726.68603515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.75_piano_model.pth\n",
      "33450.58349609375\n",
      "final loss val for 1272 is 33450.58349609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:13<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.75_piano_model.pth\n",
      "465963.85986328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:14<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.75_piano_model.pth\n",
      "190786.88916015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:13<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.75_piano_model.pth\n",
      "110232.814453125\n",
      "final loss val for 216 is 110232.814453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.75_piano_model.pth\n",
      "183594.34106445312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.75_piano_model.pth\n",
      "155945.08569335938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.75_piano_model.pth\n",
      "147962.76977539062\n",
      "final loss val for 1024 is 147962.76977539062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.75_piano_model.pth\n",
      "206484.40649414062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.75_piano_model.pth\n",
      "175190.37353515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.75_piano_model.pth\n",
      "171004.2862548828\n",
      "final loss val for 962 is 171004.2862548828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:27<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.75_piano_model.pth\n",
      "305548.1257324219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:27<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.75_piano_model.pth\n",
      "260913.677734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:27<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.75_piano_model.pth\n",
      "255317.9970703125\n",
      "final loss val for 505 is 255317.9970703125\n",
      "Classified 21758 frames of audio, classification arcuracy 0.2699252730448153, average regression error is 0.18600080707776917\n",
      "The true accuracy of classifying pressed_key are 0.9883087962390201\n",
      "Ratio of pressed is in data is 0.033772321988150646\n"
     ]
    }
   ],
   "source": [
    "model = PianoNoteModel(output_size=(88, 3))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "save_model(model, optimizer, \"prototype_nofeedback_inver1:0.75\")\n",
    "\n",
    "for index in indicies:\n",
    "    final_loss = 0\n",
    "    for i in range(3): \n",
    "        train_lost_val = one_pass_song_train(\"prototype_nofeedback_inver1:0.75\", index)\n",
    "        print(train_lost_val)\n",
    "        final_loss = train_lost_val\n",
    "    print(f\"final loss val for {index} is {final_loss}\")\n",
    "\n",
    "a, b, c = validation_accuracy_check(\"prototype_nofeedback_inver1:0.75\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba774fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.6_piano_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youxu\\AppData\\Local\\Temp\\ipykernel_9368\\3530722454.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(actualPath)\n",
      "100%|██████████| 15/15 [00:06<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.6_piano_model.pth\n",
      "970016.8449707031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.6_piano_model.pth\n",
      "134986.41967773438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.6_piano_model.pth\n",
      "95581.02197265625\n",
      "final loss val for 686 is 95581.02197265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.6_piano_model.pth\n",
      "49826.840087890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:05<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.6_piano_model.pth\n",
      "31165.735229492188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:05<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.6_piano_model.pth\n",
      "26757.059936523438\n",
      "final loss val for 1272 is 26757.059936523438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:14<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.6_piano_model.pth\n",
      "235625.57263183594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:13<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.6_piano_model.pth\n",
      "101091.244140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:14<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.6_piano_model.pth\n",
      "83953.20556640625\n",
      "final loss val for 216 is 83953.20556640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.6_piano_model.pth\n",
      "143993.48754882812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.6_piano_model.pth\n",
      "125009.18115234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.6_piano_model.pth\n",
      "121054.04602050781\n",
      "final loss val for 1024 is 121054.04602050781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.6_piano_model.pth\n",
      "164406.35400390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.6_piano_model.pth\n",
      "146316.326171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.6_piano_model.pth\n",
      "141868.64392089844\n",
      "final loss val for 962 is 141868.64392089844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:27<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.6_piano_model.pth\n",
      "244876.7774658203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:28<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.6_piano_model.pth\n",
      "220074.19274902344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:26<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.6_piano_model.pth\n",
      "211189.76904296875\n",
      "final loss val for 505 is 211189.76904296875\n",
      "Classified 21758 frames of audio, classification arcuracy 0.3747529644268776, average regression error is 0.185820740366565\n",
      "The true accuracy of classifying pressed_key are 0.9604416676976371\n",
      "Ratio of pressed is in data is 0.033772321988150646\n"
     ]
    }
   ],
   "source": [
    "\"\"\"model = PianoNoteModel(output_size=(88, 3))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "save_model(model, optimizer, \"prototype_nofeedback_inver1:0.6\")\"\"\"\n",
    "\n",
    "for index in indicies:\n",
    "    final_loss = 0\n",
    "    for i in range(3): \n",
    "        train_lost_val = one_pass_song_train(\"prototype_nofeedback_inver1:0.6\", index, 1.6)\n",
    "        print(train_lost_val)\n",
    "        final_loss = train_lost_val\n",
    "    print(f\"final loss val for {index} is {final_loss}\")\n",
    "\n",
    "a, b, c = validation_accuracy_check(\"prototype_nofeedback_inver1:0.6\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7005c981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youxu\\AppData\\Local\\Temp\\ipykernel_9368\\3530722454.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(actualPath)\n",
      "100%|██████████| 15/15 [00:06<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.5_piano_model.pth\n",
      "50881.95782470703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.5_piano_model.pth\n",
      "43366.88195800781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.5_piano_model.pth\n",
      "39543.89520263672\n",
      "final loss val for 686 is 39543.89520263672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.5_piano_model.pth\n",
      "24600.193359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.5_piano_model.pth\n",
      "20749.053100585938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.5_piano_model.pth\n",
      "18102.952880859375\n",
      "final loss val for 1272 is 18102.952880859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:14<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.5_piano_model.pth\n",
      "91381.96856689453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:14<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.5_piano_model.pth\n",
      "65971.85675048828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:14<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.5_piano_model.pth\n",
      "60683.240966796875\n",
      "final loss val for 216 is 60683.240966796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.5_piano_model.pth\n",
      "108787.751953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.5_piano_model.pth\n",
      "96747.28991699219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.5_piano_model.pth\n",
      "93067.65930175781\n",
      "final loss val for 1024 is 93067.65930175781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.5_piano_model.pth\n",
      "126934.00451660156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.5_piano_model.pth\n",
      "100564.47937011719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.5_piano_model.pth\n",
      "89366.72766113281\n",
      "final loss val for 962 is 89366.72766113281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:27<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.5_piano_model.pth\n",
      "170859.40368652344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:26<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.5_piano_model.pth\n",
      "149027.56518554688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:27<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1:0.5_piano_model.pth\n",
      "139293.43920898438\n",
      "final loss val for 505 is 139293.43920898438\n"
     ]
    }
   ],
   "source": [
    "\"\"\"model = PianoNoteModel(output_size=(88, 3))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "save_model(model, optimizer, \"prototype_nofeedback_inver1:0.5\")\"\"\"\n",
    "\n",
    "for index in indicies:\n",
    "    final_loss = 0\n",
    "    for i in range(3): \n",
    "        train_lost_val = one_pass_song_train(\"prototype_nofeedback_inver1:0.5\", index)\n",
    "        print(train_lost_val)\n",
    "        final_loss = train_lost_val\n",
    "    print(f\"final loss val for {index} is {final_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "242f8e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youxu\\AppData\\Local\\Temp\\ipykernel_9368\\3530722454.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(actualPath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified 21758 frames of audio, classification arcuracy 0.4214238858852335, average regression error is 0.18541228469871776\n",
      "The true accuracy of classifying pressed_key are 0.9379716689348014\n",
      "Ratio of pressed is in data is 0.033772321988150646\n"
     ]
    }
   ],
   "source": [
    "a, b, c = validation_accuracy_check(\"prototype_nofeedback_inver1:0.5\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b98f66a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youxu\\AppData\\Local\\Temp\\ipykernel_9368\\3530722454.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(actualPath)\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_inverse_fraction_piano_model.pth\n",
      "71897.5269165039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_inverse_fraction_piano_model.pth\n",
      "60413.78088378906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_inverse_fraction_piano_model.pth\n",
      "44338.21936035156\n",
      "final loss val for 686 is 44338.21936035156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:05<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_inverse_fraction_piano_model.pth\n",
      "31189.3662109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_inverse_fraction_piano_model.pth\n",
      "20061.83349609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_inverse_fraction_piano_model.pth\n",
      "17281.81298828125\n",
      "final loss val for 1272 is 17281.81298828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:14<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_inverse_fraction_piano_model.pth\n",
      "99193.7603149414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:14<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_inverse_fraction_piano_model.pth\n",
      "61098.87353515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:14<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_inverse_fraction_piano_model.pth\n",
      "49135.741638183594\n",
      "final loss val for 216 is 49135.741638183594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_inverse_fraction_piano_model.pth\n",
      "109132.35363769531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_inverse_fraction_piano_model.pth\n",
      "89975.22229003906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_inverse_fraction_piano_model.pth\n",
      "78415.65991210938\n",
      "final loss val for 1024 is 78415.65991210938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_inverse_fraction_piano_model.pth\n",
      "115740.12658691406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_inverse_fraction_piano_model.pth\n",
      "82643.13916015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_inverse_fraction_piano_model.pth\n",
      "68292.02655029297\n",
      "final loss val for 962 is 68292.02655029297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:27<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_inverse_fraction_piano_model.pth\n",
      "186144.37817382812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:27<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_inverse_fraction_piano_model.pth\n",
      "150751.97912597656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:26<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_inverse_fraction_piano_model.pth\n",
      "139014.96520996094\n",
      "final loss val for 505 is 139014.96520996094\n",
      "Classified 21758 frames of audio, classification arcuracy 0.5504981448829687, average regression error is 0.18436247032018957\n",
      "The true accuracy of classifying pressed_key are 0.8865520227638253\n",
      "Ratio of pressed is in data is 0.033772321988150646\n"
     ]
    }
   ],
   "source": [
    "# the ratio is np/p\n",
    "\"\"\"model = PianoNoteModel(output_size=(88, 3))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "save_model(model, optimizer, \"prototype_inverse_fraction\")\"\"\"\n",
    "\n",
    "for index in indicies:\n",
    "    final_loss = 0\n",
    "    for i in range(3): \n",
    "        train_lost_val = one_pass_song_train(\"prototype_inverse_fraction\", index, 1.0)\n",
    "        print(train_lost_val)\n",
    "        final_loss = train_lost_val\n",
    "    print(f\"final loss val for {index} is {final_loss}\")\n",
    "\n",
    "a, b, c = validation_accuracy_check(\"prototype_inverse_fraction\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "adb4b93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youxu\\AppData\\Local\\Temp\\ipykernel_9368\\3530722454.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(actualPath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified 21758 frames of audio, classification arcuracy 0.5487829972674627, average regression error is 0.18461263969832345\n",
      "The true accuracy of classifying pressed_key are 0.8737628355808487\n",
      "Ratio of pressed is in data is 0.033772321988150646\n"
     ]
    }
   ],
   "source": [
    "a, b, c = validation_accuracy_check(\"prototype_nofeedback_inver1:0.5\", 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "17d7ecc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_04_piano_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youxu\\AppData\\Local\\Temp\\ipykernel_9368\\3530722454.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(actualPath)\n",
      "100%|██████████| 15/15 [00:06<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_04_piano_model.pth\n",
      "442000.3289794922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_04_piano_model.pth\n",
      "69669.31188964844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_04_piano_model.pth\n",
      "54885.06213378906\n",
      "final loss val for 686 is 54885.06213378906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_04_piano_model.pth\n",
      "36042.288818359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:05<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_04_piano_model.pth\n",
      "26813.445068359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:05<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_04_piano_model.pth\n",
      "22851.036010742188\n",
      "final loss val for 1272 is 22851.036010742188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:14<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_04_piano_model.pth\n",
      "143046.80529785156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:14<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_04_piano_model.pth\n",
      "111749.10388183594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:13<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_04_piano_model.pth\n",
      "71905.22424316406\n",
      "final loss val for 216 is 71905.22424316406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_04_piano_model.pth\n",
      "123678.58190917969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_04_piano_model.pth\n",
      "102039.53723144531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_04_piano_model.pth\n",
      "99103.34643554688\n",
      "final loss val for 1024 is 99103.34643554688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_04_piano_model.pth\n",
      "136814.4676513672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_04_piano_model.pth\n",
      "118932.57531738281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_04_piano_model.pth\n",
      "115116.23498535156\n",
      "final loss val for 962 is 115116.23498535156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:27<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_04_piano_model.pth\n",
      "194215.29833984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:26<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_04_piano_model.pth\n",
      "174008.4061279297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:27<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_04_piano_model.pth\n",
      "166102.2537841797\n",
      "final loss val for 505 is 166102.2537841797\n",
      "Classified 21758 frames of audio, classification arcuracy 0.46278589275418036, average regression error is 0.18587493631306987\n",
      "The true accuracy of classifying pressed_key are 0.9010577755783744\n",
      "Ratio of pressed is in data is 0.033772321988150646\n"
     ]
    }
   ],
   "source": [
    "# the ratio is (np/p)**1.4\n",
    "model = PianoNoteModel(output_size=(88, 3))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "save_model(model, optimizer, \"prototype_nofeedback_inver1_04\")\n",
    "\n",
    "for index in indicies:\n",
    "    final_loss = 0\n",
    "    for i in range(3): \n",
    "        train_lost_val = one_pass_song_train(\"prototype_nofeedback_inver1_04\", index, 1.4)\n",
    "        print(train_lost_val)\n",
    "        final_loss = train_lost_val\n",
    "    print(f\"final loss val for {index} is {final_loss}\")\n",
    "\n",
    "a, b, c = validation_accuracy_check(\"prototype_nofeedback_inver1_04\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cd548de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youxu\\AppData\\Local\\Temp\\ipykernel_9368\\3530722454.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(actualPath)\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_03_piano_model.pth\n",
      "46699.016174316406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_03_piano_model.pth\n",
      "39016.36114501953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_03_piano_model.pth\n",
      "35678.28564453125\n",
      "final loss val for 686 is 35678.28564453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:05<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_03_piano_model.pth\n",
      "23940.485107421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:05<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_03_piano_model.pth\n",
      "19154.5107421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:05<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_03_piano_model.pth\n",
      "17734.93292236328\n",
      "final loss val for 1272 is 17734.93292236328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:14<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_03_piano_model.pth\n",
      "83239.20153808594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:14<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_03_piano_model.pth\n",
      "66835.99884033203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:14<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_03_piano_model.pth\n",
      "63358.31329345703\n",
      "final loss val for 216 is 63358.31329345703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_03_piano_model.pth\n",
      "108618.88684082031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_03_piano_model.pth\n",
      "88017.40283203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_03_piano_model.pth\n",
      "79217.44946289062\n",
      "final loss val for 1024 is 79217.44946289062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_03_piano_model.pth\n",
      "106870.63928222656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_03_piano_model.pth\n",
      "88532.06018066406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_03_piano_model.pth\n",
      "78810.58312988281\n",
      "final loss val for 962 is 78810.58312988281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:27<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_03_piano_model.pth\n",
      "157921.3243408203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:27<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_03_piano_model.pth\n",
      "131148.43908691406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:27<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_03_piano_model.pth\n",
      "119024.27429199219\n",
      "final loss val for 505 is 119024.27429199219\n",
      "Classified 21758 frames of audio, classification arcuracy 0.5994869180823769, average regression error is 0.1857240335261277\n",
      "The true accuracy of classifying pressed_key are 0.8078683657058023\n",
      "Ratio of pressed is in data is 0.033772321988150646\n"
     ]
    }
   ],
   "source": [
    "# the ratio is (np/p)**1.4\n",
    "\"\"\"model = PianoNoteModel(output_size=(88, 3))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "save_model(model, optimizer, \"prototype_nofeedback_inver1_03\")\"\"\"\n",
    "\n",
    "for index in indicies:\n",
    "    final_loss = 0\n",
    "    for i in range(3): \n",
    "        train_lost_val = one_pass_song_train(\"prototype_nofeedback_inver1_03\", index, 1.3)\n",
    "        print(train_lost_val)\n",
    "        final_loss = train_lost_val\n",
    "    print(f\"final loss val for {index} is {final_loss}\")\n",
    "\n",
    "a, b, c = validation_accuracy_check(\"prototype_nofeedback_inver1_03\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "076278c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youxu\\AppData\\Local\\Temp\\ipykernel_9368\\3530722454.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(actualPath)\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_02_piano_model.pth\n",
      "54125.751525878906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_02_piano_model.pth\n",
      "45380.26171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_02_piano_model.pth\n",
      "41580.21618652344\n",
      "final loss val for 686 is 41580.21618652344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:05<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_02_piano_model.pth\n",
      "25186.12353515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_02_piano_model.pth\n",
      "23226.1943359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_02_piano_model.pth\n",
      "21974.210205078125\n",
      "final loss val for 1272 is 21974.210205078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:14<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_02_piano_model.pth\n",
      "91678.52392578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:14<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_02_piano_model.pth\n",
      "67231.25909423828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:14<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_02_piano_model.pth\n",
      "62852.713928222656\n",
      "final loss val for 216 is 62852.713928222656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_02_piano_model.pth\n",
      "111874.1630859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_02_piano_model.pth\n",
      "101216.50805664062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_02_piano_model.pth\n",
      "97264.87524414062\n",
      "final loss val for 1024 is 97264.87524414062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_02_piano_model.pth\n",
      "133047.2872314453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_02_piano_model.pth\n",
      "109820.064453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_02_piano_model.pth\n",
      "102901.66589355469\n",
      "final loss val for 962 is 102901.66589355469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:27<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_02_piano_model.pth\n",
      "177592.96032714844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:27<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_02_piano_model.pth\n",
      "165854.9178466797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:27<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/prototype_nofeedback_inver1_02_piano_model.pth\n",
      "161723.16430664062\n",
      "final loss val for 505 is 161723.16430664062\n",
      "Classified 21758 frames of audio, classification arcuracy 0.47476424554395863, average regression error is 0.18519864102936984\n",
      "The true accuracy of classifying pressed_key are 0.9035475689719163\n",
      "Ratio of pressed is in data is 0.033772321988150646\n"
     ]
    }
   ],
   "source": [
    "\"\"\"model = PianoNoteModel(output_size=(88, 3))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "save_model(model, optimizer, \"prototype_nofeedback_inver1_02\")\"\"\"\n",
    "\n",
    "for index in indicies:\n",
    "    final_loss = 0\n",
    "    for i in range(3): \n",
    "        train_lost_val = one_pass_song_train(\"prototype_nofeedback_inver1_02\", index, 1.2)\n",
    "        print(train_lost_val)\n",
    "        final_loss = train_lost_val\n",
    "    print(f\"final loss val for {index} is {final_loss}\")\n",
    "\n",
    "a, b, c = validation_accuracy_check(\"prototype_nofeedback_inver1_02\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708d2778",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
