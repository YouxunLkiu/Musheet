{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f80e4451",
   "metadata": {},
   "source": [
    "# Piano to Sheet\n",
    "\n",
    "Converting .wav piano pieces into music sheets.\n",
    "\n",
    "I\n",
    "\n",
    "## Pipeline break down ( beta)\n",
    "\n",
    "1. Loading in the wave file,\n",
    "2. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941fd603",
   "metadata": {},
   "source": [
    "## Audio Processing  Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e630b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\youxu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaff679",
   "metadata": {},
   "source": [
    "For the processing libraries, we will be using pretty_midi to extract out label information from the given midi files.  \n",
    "And for processing our wav audio files. We will be using librosa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c76f67",
   "metadata": {},
   "source": [
    "## Machine Learning Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85047046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import librosa \n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pretty_midi as pm\n",
    "import mido\n",
    "import IPython.display as ipd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a26ef70",
   "metadata": {},
   "source": [
    "# Model Saving funtion and Model loading function\n",
    "\n",
    "Having a model saving function and a modle loading function in which we can train the model in small epoch progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cccab19",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ef91a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_file_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/maestro-v3.0.0.json\"\n",
    "with open(labels_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "all_sets = {}\n",
    "all_sets['train'] = []\n",
    "all_sets['validation'] = []\n",
    "all_sets['test'] = []\n",
    "\n",
    "def sortingsets (data, allsets):\n",
    "    for key in data:\n",
    "       \n",
    "        if data[key] == 'train':\n",
    "            all_sets['train'].append(key)\n",
    "        elif data[key] == 'validation':\n",
    "            all_sets['validation'].append(key)\n",
    "        else:\n",
    "            all_sets['test'].append(key)\n",
    "\n",
    "def save_index_to_csv(all_sets):\n",
    "    for key in all_sets:\n",
    "        path = f\"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/{key}_indicies.csv\"\n",
    "        df = pd.DataFrame({'Index': all_sets[key]})\n",
    "        df.to_csv(path, index=False)\n",
    "\n",
    "def load_index_from_csv(path):\n",
    "    indices = []\n",
    "    with open(path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            indices.append(int(row[0]))\n",
    "    return indices\n",
    "\n",
    "def save_progress_index_to_csv(indices, epoch):\n",
    "    path = f\"models/training_index{epoch}.csv\"\n",
    "    df = pd.DataFrame({'Index': indices})\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "def load_progress_index_from_csv(epoch):\n",
    "    path = f\"models/training_index{epoch}.csv\"\n",
    "    indices = []\n",
    "    with open(path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            indices.append(int(row[0]))\n",
    "    return indices\n",
    "\n",
    "sortingsets(data['split'], all_sets)\n",
    "save_index_to_csv(all_sets)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1870b3aa",
   "metadata": {},
   "source": [
    "traindata/maestro-v3.0.0-midi/maestro-v3.0.0/train_indicies.csv  \n",
    "traindata/maestro-v3.0.0-midi/maestro-v3.0.0/test_indicies.csv   \n",
    "traindata/maestro-v3.0.0-midi/maestro-v3.0.0/validation_indicies.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0440f3",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "implenting utility functions such as the randomizing the data set for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d594a34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### making the random seed \n",
    "np.random.seed(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77d296ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomizing the data set index for training purposes\n",
    "def randomizeing(data_set):\n",
    "    ds = np.array(data_set)\n",
    "    np.random.shuffle(ds)\n",
    "    return ds\n",
    "\n",
    "## Select n indices from the givien data set\n",
    "def select_N_randomized_from_set(n, data_set):\n",
    "    nparry = randomizeing(data_set)\n",
    "    return nparry[:n]\n",
    "\n",
    "\n",
    "## ----- ----- ---------- loading function ------------------------------ ##\n",
    "\n",
    "## function loading in the wav function\n",
    "def load_wav_from_index(index):\n",
    "    labels_file_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/maestro-v3.0.0.json\"\n",
    "    with open(labels_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    wav_path = \"traindata/maestro-v3.0.0/maestro-v3.0.0/\" + data['audio_filename'][str(index)]\n",
    "    \n",
    "    return librosa.load(wav_path, sr=None)\n",
    "\n",
    "## function loading in the midi function\n",
    "def load_midi_from_index(index):\n",
    "    labels_file_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/maestro-v3.0.0.json\"\n",
    "    with open(labels_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    midi_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/\" + data['midi_filename'][str(index)]\n",
    "    return pm.PrettyMIDI(midi_path)\n",
    "\n",
    "## ----- ----- -------- Path showing function-------------------------------- ##\n",
    "    \n",
    "## showing the file path audio of the wave\n",
    "def show_wav_path(index):\n",
    "    labels_file_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/maestro-v3.0.0.json\"\n",
    "    with open(labels_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    wav_path = \"traindata/maestro-v3.0.0/maestro-v3.0.0/\" + data['audio_filename'][str(index)]\n",
    "    return wav_path\n",
    "\n",
    "## Showing the file path of the midi file of data[index]\n",
    "def show_midi_path(index):\n",
    "    labels_file_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/maestro-v3.0.0.json\"\n",
    "    with open(labels_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    midi_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/\" + data['midi_filename'][str(index)]\n",
    "    return midi_path\n",
    "\n",
    "## loading in the wav and midi pair\n",
    "def load_wav_midi_pair(index): ## (wav, midi)\n",
    "    return load_wav_from_index(index), load_midi_from_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1db3f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "train_data = load_index_from_csv(\"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/train_indicies.csv\")\n",
    "print(len(train_data))\n",
    "subset_train_data = select_N_randomized_from_set(50, train_data)\n",
    "\n",
    "print(len(subset_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0651ce0a",
   "metadata": {},
   "source": [
    "# AUDIO EXPERIMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abeb97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(wav, sr), midi = load_wav_midi_pair(1025)\n",
    "# midi = load_midi_from_index(1025)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5babb367",
   "metadata": {},
   "outputs": [],
   "source": [
    "ballade1, sr = load_wav_from_index(505)\n",
    "\n",
    "noised_ballade1 = add_gaussian_noise(ballade1, noise_level=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b0d34459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24082675\n"
     ]
    }
   ],
   "source": [
    "print(len(ballade1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "24fdace7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24082675\n"
     ]
    }
   ],
   "source": [
    "print(len(noised_ballade1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf86bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(data= ballade1, rate= sr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c3118",
   "metadata": {},
   "source": [
    "### Audio Preprocessing functions\n",
    "\n",
    "#### Pretty_midi note\n",
    "The MIDI object is used from the python package pretty_midi.  \n",
    "Using pretty_midi range from 0 to 127. we can later tranform this into the range of 0 to 87 to match a piano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc1cefca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\"\"\" This function extracts all played notes in the midi Object, which it will be futher trained with the aligne ed wave object\n",
    "    input: pm object\n",
    "    It is good for debugging and seeing the midi object\n",
    "\"\"\"\n",
    "def extract_midi_notes(midi):\n",
    "    notes = []\n",
    "    for instrument in midi.instruments:\n",
    "        for note in instrument.notes:\n",
    "            notes.append({\n",
    "                'pitch': note.pitch,\n",
    "                'start': note.start,\n",
    "                'end': note.end,\n",
    "                'velocity': note.velocity\n",
    "            })\n",
    "    #preprocessing the sort\n",
    "    notes.sort(key=(lambda x: x['start']))\n",
    "    return notes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Wrting a peekable Generator for midi object\"\"\"\n",
    "class PeekableGenerator:\n",
    "    def __init__(self, generator):\n",
    "        self._generator = generator\n",
    "        self._next_item = None\n",
    "        self._has_next = False\n",
    "        self._advance()\n",
    "\n",
    "    def _advance(self):\n",
    "        try:\n",
    "            self._next_item = self._generator.__next__()\n",
    "            self._has_next = True\n",
    "        except StopIteration:\n",
    "            self._next_item = None\n",
    "            self._has_next = False\n",
    "\n",
    "    def peek(self):\n",
    "        if not self._has_next:\n",
    "            raise StopIteration(\"No more elements to peek at.\")\n",
    "        return self._next_item\n",
    "\n",
    "    def __next__(self):\n",
    "        if not self._has_next:\n",
    "            raise StopIteration(\"No more elements.\")\n",
    "        current = self._next_item\n",
    "        self._advance()\n",
    "        return current\n",
    "\n",
    "    def has_next(self):\n",
    "        return self._has_next\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield self._next_item\n",
    "        self._advance()\n",
    "\n",
    "    \n",
    "\n",
    "\"\"\"Generator to yield midi note object at the frame during the classification\n",
    "    input: pm object\n",
    "\"\"\"\n",
    "def midi_yielding(midi):\n",
    "    all_midi_obj :list = extract_midi_notes(midi)\n",
    "    ##Processing\n",
    "    for note in all_midi_obj:\n",
    "        yield note\n",
    "\n",
    "\n",
    "\"\"\" Yielding a list of midi notes information where it fits the time frame automatically.\n",
    "    Implemented using overlapping frame structure for the training.\n",
    "    Begin at 0, the frame jumping at the speed of jump_len, the size of the frame is frame_len\n",
    "    This function will yield the frame at the given parameter.\n",
    "\"\"\"\n",
    "def frame_aligning_midi(t: int, jump_len:int, frame_len: int, sr:int, midi):\n",
    "    midi_generator = PeekableGenerator(midi_yielding(midi))\n",
    "    midi_labels = []\n",
    "\n",
    "    jump_time_fraction: float = jump_len * (1/sr)\n",
    "    frame_time_fraction: float = frame_len * (1/sr)\n",
    "    framing = [t*jump_time_fraction, t*jump_time_fraction + frame_time_fraction]\n",
    "    \n",
    "    last_note = 0\n",
    "    while midi_generator.has_next() or last_note > framing[0]:\n",
    "        \n",
    "        while midi_generator.has_next() and midi_generator.peek()['start'] >= framing[0] and midi_generator.peek()['start'] < framing[1]:\n",
    "            try:\n",
    "                midi_labels.append(midi_generator.__next__())\n",
    "            except StopIteration:\n",
    "                print(\"Generator exhausted, no Midi Objectis being added to the label\")\n",
    "                break\n",
    "        \n",
    "        ## Yielding the list of midi notes that are fitted in side the frame\n",
    "        yield midi_labels\n",
    "\n",
    "        midi_labels.sort(key=(lambda x: x['end']))\n",
    "        ##calculating the next frame time step and removing the items from the previous frame\n",
    "        framing [0] += jump_time_fraction\n",
    "        framing [1] += jump_time_fraction\n",
    "        while len(midi_labels) > 0 and midi_labels[0]['end'] < framing[0]:\n",
    "            midi_labels.pop(0)\n",
    "        \n",
    "        if len(midi_labels) > 0 and midi_labels[0]['end'] > framing[0]:\n",
    "            last_note = midi_labels[0]['end']\n",
    "\n",
    "    \n",
    "        \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\" This function returns the aligned frame at the wav data,\"\"\"\n",
    "def frame_aligning_wav(t: int, jump_len: int, frame_len: int, wav):\n",
    "    begin = t * jump_len\n",
    "    return wav[begin: begin + frame_len]\n",
    "\n",
    "\n",
    "\"\"\" This function returns the amount seconds of audio data from the wav, began on t, while using  \"\"\"\n",
    "\n",
    "def audio_segment_of(t: int, wav, seconds: float, sr: int, jump_len: int = 512, frame_len: int = 2048, ):\n",
    "    size = int(seconds*sr)\n",
    "    begin = t*jump_len\n",
    "    \n",
    "    return wav[begin: begin + size]\n",
    "\n",
    "def audio_segment_between(begin, end, wav, sr):\n",
    "    return wav[int(begin*sr): int(end*sr)]\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260c0e13",
   "metadata": {},
   "source": [
    "### Constructing Mel Spectrogram From wav frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8191371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mel_spectrogram(audio, sr, n_mels=88, hop_length=512, n_fft=4096): ##_fft is the frame length \n",
    "    \"\"\"\n",
    "    Extract a mel-spectrogram from raw audio.\n",
    "    \"\"\"\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio, sr=sr, n_mels=n_mels, hop_length=hop_length, n_fft=n_fft\n",
    "    )\n",
    "    # Convert to log scale\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return mel_spec_db\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04b5e6a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example audio input\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m frame_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[43msr\u001b[49m\n\u001b[0;32m      3\u001b[0m num_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m22\u001b[39m\n\u001b[0;32m      4\u001b[0m audio1 \u001b[38;5;241m=\u001b[39m audio_segment_between(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,ballade1, sr)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sr' is not defined"
     ]
    }
   ],
   "source": [
    "# Example audio input\n",
    "frame_time = 2048 /sr\n",
    "num_frame = 22\n",
    "audio1 = audio_segment_between(0,2,ballade1, sr)\n",
    "quick_sample = audio_segment_between(0, num_frame*frame_time, ballade1, sr)\n",
    "mel_spectrogram = extract_mel_spectrogram(audio1, sr, n_mels =188)\n",
    "print(mel_spectrogram.shape)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "librosa.display.specshow(mel_spectrogram, \n",
    "                         sr=sr, \n",
    "                         x_axis=\"linear\")\n",
    "plt.colorbar(format=\"%+2.f\")\n",
    "plt.show()\n",
    "ipd.Audio(data= audio1, rate= sr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d73318",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e9eb5b8",
   "metadata": {},
   "source": [
    "### Calculating the added noise level to the wav function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10ea68b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_noise_power(audio, snr_dB):\n",
    "    \"\"\"\n",
    "    Compute the noise power needed for a given SNR in dB.\n",
    "    :param audio: NumPy array of the audio signal.\n",
    "    :param snr_dB: Desired Signal-to-Noise Ratio in dB.\n",
    "    :return: Noise power.\n",
    "    \"\"\"\n",
    "    # Calculate signal power (mean squared amplitude)\n",
    "    signal_power = np.mean(audio ** 2)\n",
    "    \n",
    "    # Convert SNR from dB to linear scale\n",
    "    snr_linear = 10 ** (snr_dB / 10)\n",
    "    \n",
    "    # Calculate noise power\n",
    "    noise_power = signal_power / snr_linear\n",
    "    return noise_power\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24c50b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(audio, noise_level=0.0006):\n",
    "    \n",
    "    noise = np.random.normal(0, noise_level, audio.shape)\n",
    "    return audio + noise\n",
    "\n",
    "def framelining (time, jump, sr):\n",
    "    \n",
    "    frametime = jump/sr\n",
    "    print(22*frametime)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c59f752",
   "metadata": {},
   "source": [
    "### Model for musheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a81def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PianoNoteModel(nn.Module):\n",
    "    def __init__(self, num_mel_bins=88, mel_temporal_length=89, num_frame_output=22, output_size=(88, 3)):\n",
    "        \"\"\"The default parameter is approximated for 1 seconds of audio data, regarding to the temporal_length\"\"\"\n",
    "        super(PianoNoteModel, self).__init__()\n",
    "\n",
    "        self.num_mel_bins = num_mel_bins\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        # Compute flattened size based on input dimensions after pooling\n",
    "        # Assuming input shape is (batch_size, 1, num_mel_bins, num_frames)\n",
    "        pooled_mel_bins = num_mel_bins // 2  # Adjust based on pooling\n",
    "        pooled_temporal_length = mel_temporal_length // 2     # Adjust based on pooling\n",
    "\n",
    "        \n",
    "        flattened_size = pooled_mel_bins * pooled_temporal_length * 64  # Based on conv2 output channels\n",
    "        \n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(flattened_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_frame_output * output_size[0] * output_size[1])  # Predict for each frame\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, 1, num_mel_bins, num_frames)\n",
    "        \n",
    "        # Convolutional layers\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "\n",
    "       \n",
    "\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten except batch dimension\n",
    "       \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "       \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Reshape to output dimensions: (batch_size, num_frames, 88, 3)\n",
    "        x = x.view(x.size(0), -1, 88, 3)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "# Custom loss function, for mutipurpose loss function in the output layer\n",
    "class MultiTaskLoss(nn.Module):\n",
    "    def __init__(self, classification_weight=1.0, regression_weight=1.0):\n",
    "        super(MultiTaskLoss, self).__init__()\n",
    "        self.classification_loss = nn.CrossEntropyLoss()\n",
    "        self.regression_loss = nn.MSELoss()\n",
    "        self.classification_weight = classification_weight\n",
    "        self.regression_weight = regression_weight\n",
    "\n",
    "    def forward(self, classification_output, classification_target, \n",
    "                regression_output, regression_target):\n",
    "        # Compute classification loss\n",
    "        class_loss = self.classification_loss(classification_output, classification_target)\n",
    "        \n",
    "        # Compute regression loss\n",
    "        reg_loss = self.regression_loss(regression_output, regression_target)\n",
    "        \n",
    "        # Combine with weights\n",
    "        total_loss = self.classification_weight * class_loss + self.regression_weight * reg_loss\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04663d46",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b78ededb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm\n",
    "\n",
    "## MODEL Saver\n",
    "def save_model(model, optimizer, id, path = \"models/\"):\n",
    "    \"\"\" Saving the model after each training/testing before each training progress\"\"\"\n",
    "    actualPath = path + f\"{id}_piano_model.pth\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, actualPath)\n",
    "    print(f\"Model saved to {actualPath}\")\n",
    "\n",
    "## MODEL loader\n",
    "def load_model(model, optimizer, id, path = \"models/\"):\n",
    "    actualPath = path + f\"{id}_piano_model.pth\"\n",
    "    checkpoint = torch.load(actualPath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "\"\"\"This function generates the matrix label to aligne the mal_spectral gram\n",
    "each matrix should consired num_frame,\n",
    "poping fram_jumping after the matrix is being yield\n",
    "\n",
    "jumping time should be\n",
    "frame_jumping * (frame_len /sr)\n",
    "\"\"\"\n",
    "def label_generator(num_frame, frame_jumping, jump_len:int, frame_len: int, sr:int, midi):\n",
    "    midi_frame_gen = PeekableGenerator(frame_aligning_midi(0, frame_len, frame_len, sr, midi)) ## Note, while using the architechture of mel_spectrogram, the we don't need to consider the jump offset in the midi side\n",
    "    label = []\n",
    "    counter = 0\n",
    "    \n",
    "    \n",
    "    concur_time = 0\n",
    "    concur_time_end = 0\n",
    "    frame_time = frame_len /sr\n",
    "\n",
    "    \n",
    "    while midi_frame_gen.has_next():\n",
    "        if (len(label) < num_frame):\n",
    "            label.append(list(midi_frame_gen.__next__()))\n",
    "            concur_time_end += frame_time\n",
    "            \n",
    "        else:\n",
    "            counter+=1\n",
    "            yield label\n",
    "            for i in range(int(frame_jumping)):\n",
    "                label.pop(0)\n",
    "                concur_time += frame_time\n",
    "                \n",
    "\n",
    "\n",
    "\"\"\"This function tries to mimic the decayed velocity miniking the sound at which a piano has been decayed\"\"\"\n",
    "def velocity_decay_sustain (velocity, onset, at_time):\n",
    "    if (at_time - onset < 0.2):\n",
    "        return velocity\n",
    "    else:\n",
    "        return math.exp((onset - at_time) * 0.6) * velocity\n",
    "\n",
    "\n",
    "\"\"\"formating the label of list, in to a matrix of 88 * 3 matrix.\n",
    "    each row represent a strikable key, \n",
    "    column 1 (being stricked) : 0, 1 (classification purpose)\n",
    "    column 2 (onset timer) : the set of positive interger that is less than onset. (Regression purpose)\n",
    "    column 3 (velocity of which is being stricked) : the set of positive integer that is less than onset. (Regression purpose) \n",
    "        note for the velocity of the piano key will be approximately alingned with a decay parameter\"\"\"\n",
    "def label_formater(label, frameonset):\n",
    "    ret_label = np.zeros((len(label), 88,3))\n",
    "\n",
    "    for i in range(len(label)) :\n",
    "        for key_obj in label[i]:\n",
    "            pitch = key_obj['pitch'] - 20\n",
    "            ret_label[i][pitch][0] = 1\n",
    "            ret_label[i][pitch][1] = key_obj['start'] - frameonset if key_obj['start'] > frameonset else 0\n",
    "            ret_label[i][pitch][2] = velocity_decay_sustain(key_obj['velocity'],  key_obj['start'], frameonset) \n",
    "\n",
    "    return ret_label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"This function assure the input for the training will retain the dimension in the case of track is ending.\"\"\"\n",
    "def construct_input(spectrogram, x, y):\n",
    "    if spectrogram.shape == (x,y):\n",
    "        return spectrogram\n",
    "    else: \n",
    "        ret = np.zeros((x,y))\n",
    "        \n",
    "       \n",
    "        for i in range(spectrogram.shape[0]):\n",
    "            ret[i][:spectrogram.shape[1]] = spectrogram[i][:spectrogram.shape[1]]\n",
    "        return ret\n",
    "\n",
    "def train_segment(wav, sr, length=1, hop=0.5):\n",
    "    ret = []\n",
    "    max = len(wav)/sr\n",
    "    begin = 0\n",
    "    while (begin < max):\n",
    "        adding = length if begin + length < max else max - begin\n",
    "        ret.append((begin, begin + adding))\n",
    "        begin += hop\n",
    "\n",
    "    return ret\n",
    "    \n",
    "\n",
    "def label_buffer(num_frame, label):\n",
    "    if len(label) < num_frame:\n",
    "        for i in range(num_frame- len(label)):\n",
    "            label.append([])\n",
    "    return label\n",
    "\n",
    "\n",
    "\"\"\" This function will generate all the mel_spectrogram and label pair for each audio segment of the song at index\"\"\"\n",
    "def generate_data_label(index, noised=True, num_frame=22, segment_jump = 0.5, frame_length = 2048, hop_length = 512):\n",
    "    # fetching infomation\n",
    "    (wav, sr), midi = load_wav_midi_pair(index)\n",
    "    \n",
    "    if noised:\n",
    "        wav = add_gaussian_noise(wav)\n",
    "\n",
    "    frame_time =  frame_length/sr\n",
    "    quick_sample = audio_segment_between(0, num_frame*frame_time, wav, sr)\n",
    "    x, y = extract_mel_spectrogram(quick_sample, sr).shape\n",
    "    segments = train_segment(wav, sr, length = num_frame * frame_time, hop= num_frame * frame_time/2)\n",
    "    \n",
    "    label_gen = PeekableGenerator(label_generator(num_frame, num_frame*segment_jump, hop_length, frame_length, sr, midi))\n",
    "    \n",
    "    input_datas = []\n",
    "    labels = []\n",
    "    for beg, end in segments:\n",
    "        \n",
    "        if (label_gen.has_next() is False) :\n",
    "            break\n",
    "\n",
    "        audio = audio_segment_between(beg, end, wav, sr)\n",
    "        mel_spectrogram = extract_mel_spectrogram(audio, sr, hop_length= hop_length, n_fft=frame_length)\n",
    "        input_data = construct_input(mel_spectrogram, x,y)\n",
    "        input_data = np.array(input_data)\n",
    "        input_data = np.expand_dims(input_data, axis=0)\n",
    "\n",
    "        label = label_buffer(num_frame, label_gen.__next__())\n",
    "        label = label_formater(label, beg)\n",
    "        label = np.array(label)\n",
    "\n",
    "        input_datas.append(input_data)\n",
    "        labels.append(label)\n",
    "        \n",
    "    return np.array(input_datas), np.array(labels)\n",
    "   \n",
    "def generate_data(index, num_frame=22, segment_jump = 0.5, frame_length = 2048, hop_length = 512):\n",
    "    \"\"\"This function generates the audio mel_spectrogram for the song at {index} with the defaulted training parameter\n",
    "        this function is generally used for validation process.\n",
    "    \"\"\"\n",
    "    # fetching infomation\n",
    "    (wav, sr), midi = load_wav_midi_pair(index)\n",
    "    wav, sr = load_wav_from_index(index)\n",
    "    \n",
    "    frame_time =  frame_length/sr\n",
    "    quick_sample = audio_segment_between(0, num_frame*frame_time, wav, sr)\n",
    "    x, y = extract_mel_spectrogram(quick_sample, sr).shape\n",
    "    segments = train_segment(wav, sr, length = num_frame * frame_time, hop= num_frame * frame_time/2)\n",
    "    input_datas = []\n",
    " \n",
    "    for beg, end in segments:\n",
    "        audio = audio_segment_between(beg, end, wav, sr)\n",
    "        mel_spectrogram = extract_mel_spectrogram(audio, sr, hop_length= hop_length, n_fft=frame_length)\n",
    "        input_data = construct_input(mel_spectrogram, x,y)\n",
    "        input_data = np.array(input_data)\n",
    "        input_data = np.expand_dims(input_data, axis=0)   \n",
    "        input_datas.append(input_data)\n",
    "        \n",
    "    return np.array(input_datas)\n",
    "\n",
    "\n",
    "\n",
    "def one_pass_song_train(id, index, noised=True, num_frame=22, segment_jump = 0.5, frame_length = 2048, hop_length = 512, batch_size=16):\n",
    "    #load model\n",
    "    \"\"\"The training is based on how many frame should be trained at a time,\n",
    "        the default setting is suited for the expriment set up above,\n",
    "        num_frame is tried to aligned it to approx 1 second of the sample\n",
    "        segment_jumping would be trying to get 50% of the sample audio\n",
    "        The dimension of the input is:\n",
    "        batchsize * channel * \n",
    "    \"\"\"\n",
    "\n",
    "    model = PianoNoteModel()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    #Letting the classification utility be more potent than the regression utility.\n",
    "    criterion = MultiTaskLoss(classification_weight=1.7, regression_weight=0.5)\n",
    " \n",
    "    \n",
    "    model, optimizer = load_model(model, optimizer, id)\n",
    "\n",
    "    # generate all data\n",
    "    datas, labels = generate_data_label(index, noised=noised, num_frame=num_frame, segment_jump=segment_jump, frame_length=frame_length, hop_length=hop_length)\n",
    "    datas = torch.tensor(datas, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    dataset = TensorDataset(datas, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size = batch_size, shuffle =True)\n",
    "\n",
    "    \n",
    "    progress_bar = tqdm.tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    # Let model be in training mode\n",
    "    model.train()\n",
    "\n",
    "    lossval = 0.0\n",
    "    for _, (batch_inputs, batch_labels) in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        outputs = model(batch_inputs)\n",
    "        classification_output = outputs[..., 0:1]\n",
    "        regression_output = outputs[..., 1:3]\n",
    "        \n",
    "        classification_target = batch_labels[..., 0:1]\n",
    "        regression_target = batch_labels[...,1:3]\n",
    "        loss = criterion.forward(classification_output, classification_target, regression_output, regression_target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        lossval+= loss.item()\n",
    "       \n",
    "        \n",
    "    #save model\n",
    "    save_model(model, optimizer, id)\n",
    "    return lossval\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_N_song_on_epoch (epoch, N, noised=True):\n",
    "    \n",
    "\n",
    "    tobe_done = load_progress_index_from_csv(epoch)\n",
    "\n",
    "    ## indices to be trained for the current run\n",
    "    parse_in = [tobe_done.pop() for i in range(N)]\n",
    "\n",
    "    \n",
    "    ## Passing in each index to train on one_pass_song_train\n",
    "    for index in parse_in:\n",
    "        one_pass_song_train()\n",
    "\n",
    "    print(\"Message from train_N_song_on_epoch\") \n",
    "    if (len(tobe_done) == 0):\n",
    "        print(f\"Epoch {epoch} training complete\")\n",
    "    else :\n",
    "        \n",
    "        print(f\"Epoch {epoch} have trained {N} songs, left over songs for current epoch will be saved. Need to train {len(tobe_done)}.\")\n",
    "        save_progress_index_to_csv(tobe_done, epoch)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d5066c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b42ac20",
   "metadata": {},
   "source": [
    "### Index Processing\n",
    "\n",
    "Using the saved indices we have processed before  \n",
    "Here are the file paths  \n",
    "for training datas :traindata/maestro-v3.0.0-midi/maestro-v3.0.0/train_indicies.csv  \n",
    "for test datas :traindata/maestro-v3.0.0-midi/maestro-v3.0.0/test_indicies.csv   \n",
    "for validataion datas :traindata/maestro-v3.0.0-midi/maestro-v3.0.0/validation_indicies.csv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "64563c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only run this once\n",
    "train_indices = load_index_from_csv(\"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/train_indicies.csv\")\n",
    "validation_indices = load_index_from_csv(\"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/validation_indicies.csv\")\n",
    "test_indices = load_index_from_csv(\"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/test_indicies.csv\")\n",
    "\n",
    "def initiate_train_epoch(epoch_Number, dataset):\n",
    "    train_indices = load_index_from_csv(dataset)\n",
    "    train_indices = randomizeing(train_indices)\n",
    "    save_progress_index_to_csv(train_indices, epoch_Number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc9808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/test_piano_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Initiate the training epoch. RUN ONCE BEFORE TRAINING\n",
    "#initiate_train_epoch(0, \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/train_indicies.csv\")\n",
    "\n",
    "# Initiate the training model. RUN ONCE BEFORE TRAINING\n",
    "model = PianoNoteModel(output_size=(88, 3))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "save_model(model, optimizer, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f318a2c7",
   "metadata": {},
   "source": [
    "## Experimenting repetitive training \n",
    "Experimenting training on one single sample to see if there are any progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9aa7cc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicies = [686, 1272, 216, 1024, 962, 505]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45da5c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/test_lr_001_e_5_piano_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youxu\\AppData\\Local\\Temp\\ipykernel_9932\\4073736929.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(actualPath)\n",
      "100%|██████████| 15/15 [00:06<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/test_lr_001_e_5_piano_model.pth\n",
      "3421.7944946289062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/test_lr_001_e_5_piano_model.pth\n",
      "663.8975715637207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/test_lr_001_e_5_piano_model.pth\n",
      "673.4657096862793\n",
      "final loss val for 686 is 673.4657096862793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:05<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/test_lr_001_e_5_piano_model.pth\n",
      "319.96841049194336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:05<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/test_lr_001_e_5_piano_model.pth\n",
      "313.3705949783325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:06<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/test_lr_001_e_5_piano_model.pth\n",
      "307.6053810119629\n",
      "final loss val for 1272 is 307.6053810119629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:13<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/test_lr_001_e_5_piano_model.pth\n",
      "870.340763092041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:14<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/test_lr_001_e_5_piano_model.pth\n",
      "847.5814266204834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:14<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/test_lr_001_e_5_piano_model.pth\n",
      "843.931637763977\n",
      "final loss val for 216 is 843.931637763977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/test_lr_001_e_5_piano_model.pth\n",
      "1361.969476699829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/test_lr_001_e_5_piano_model.pth\n",
      "1333.9757995605469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:16<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/test_lr_001_e_5_piano_model.pth\n",
      "1329.4253635406494\n",
      "final loss val for 1024 is 1329.4253635406494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/test_lr_001_e_5_piano_model.pth\n",
      "1211.3747863769531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/test_lr_001_e_5_piano_model.pth\n",
      "1166.2812900543213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:17<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/test_lr_001_e_5_piano_model.pth\n",
      "1142.66623878479\n",
      "final loss val for 962 is 1142.66623878479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:26<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/test_lr_001_e_5_piano_model.pth\n",
      "1548.3043251037598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:26<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/test_lr_001_e_5_piano_model.pth\n",
      "1532.4745655059814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:27<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/test_lr_001_e_5_piano_model.pth\n",
      "1518.1284284591675\n",
      "final loss val for 505 is 1518.1284284591675\n"
     ]
    }
   ],
   "source": [
    "model = PianoNoteModel(output_size=(88, 3))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "save_model(model, optimizer, \"test_lr_001_e_5\")\n",
    "\n",
    "for index in indicies:\n",
    "    final_loss = 0\n",
    "    for i in range(3): \n",
    "        train_lost_val = one_pass_song_train(\"test_lr_001_e_5\", index)\n",
    "        print(train_lost_val)\n",
    "        final_loss = train_lost_val\n",
    "    print(f\"final loss val for {index} is {final_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5225c5f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fee343f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5e5b2bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressbar():\n",
    "    iterator = [i for i in range(1000)]\n",
    "    for i in tqdm.tqdm(iterator):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb22de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505722a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1065, 1, 88, 89)\n",
      "(1065, 1, 88, 89)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ce08ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03cee572",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f63d9dec",
   "metadata": {},
   "source": [
    "### Validation functions\n",
    "\n",
    "Using these functions to check the acuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a98dfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validation_accuracy_check(id, index, noised=True, num_frame=22, segment_jump = 0.5, frame_length = 2048, hop_length = 512, batch_size=16):\n",
    "    \"\"\"This function returns the accuracy of the model predicting the song at index\n",
    "        arguments:\n",
    "            id: trained model id\n",
    "            index: index of the song.\n",
    "    \"\"\"\n",
    "    model = PianoNoteModel()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    #Letting the classification utility be more potent than the regression utility.\n",
    "    criterion = MultiTaskLoss(classification_weight=1.5, regression_weight=0.7)\n",
    " \n",
    "    \n",
    "    model, optimizer = load_model(model, optimizer, id)\n",
    "   \n",
    "\n",
    "    datas, labels = generate_data_label(index, num_frame=num_frame, segment_jump=segment_jump, frame_length=frame_length, hop_length=hop_length)\n",
    "    \n",
    "\n",
    "    datas = torch.tensor(datas, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    dataset = TensorDataset(datas, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    classification_correct = 0\n",
    "    classification_true_correct = 0\n",
    "    regression_error = 0\n",
    "\n",
    "    true_classified = 0 \n",
    "    num_classified = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for val_inputs, val_labels in dataloader:\n",
    "            outputs = model(val_inputs)\n",
    "\n",
    "            #Output separation\n",
    "            classification_outputs = outputs[:,:,:, 0:1].squeeze(-1)\n",
    "            regression_outputs = outputs[:, :, :, 1:3]\n",
    "\n",
    "            #Label separation\n",
    "            classification_labels = val_labels[:, :, :, 0:1].squeeze(-1)\n",
    "            regression_labels = val_labels[:, :, :, 1:3]\n",
    "            \n",
    "            #Calculate the classification accuracy of a frame\n",
    "            predicted_classes = (classification_outputs > 0.5).float()\n",
    "            classification_correct += (predicted_classes == classification_labels).sum().item() / model.num_mel_bins #this should be the accuracy of a frame\n",
    "\n",
    "\n",
    "            #Calculate the classification accuracy of pressed key\n",
    "            pressed_key_mask = classification_labels == 1\n",
    "            pressed_key_label = classification_labels[pressed_key_mask]\n",
    "            pressed_key_prediction = predicted_classes[pressed_key_mask]\n",
    "            true_classified += pressed_key_label.numel()\n",
    "            classification_true_correct += (pressed_key_label == pressed_key_prediction).sum().item()\n",
    "            \n",
    "\n",
    "            regression_error += ((regression_outputs - regression_labels) ** 2).mean().item()\n",
    "            \n",
    "            num_classified += val_labels.shape[0] * val_labels.shape[1] # counting number of frames have been classified i.e. batch_number * number of frame per segment\n",
    "            \n",
    "\n",
    "    if true_classified > 0 :\n",
    "        true_accuracy = classification_true_correct/true_classified\n",
    "    else:\n",
    "        true_accuracy = 0\n",
    "    print(f\"Classified {num_classified} frames of audio, classification arcuracy {classification_correct/ num_classified}, average regression error is {regression_error/ num_classified}\")\n",
    "    print(f\"The true accuracy of classifying pressed_key are {true_accuracy}\")\n",
    "    return num_classified, classification_correct, regression_error\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def model_output(id, index):\n",
    "    \"\"\" This function returns the audio classification from the model {id}\n",
    "        arguments : id (model id)\n",
    "                    index (song index)\n",
    "            return: classification result\n",
    "    \"\"\"\n",
    "    model = PianoNoteModel()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    model, _ = load_model(model, optimizer, id)\n",
    "    audio_data = generate_data(index)\n",
    "\n",
    "\n",
    "    datas = torch.tensor(audio_data, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    dataloader = DataLoader(audio_data, batch_size = 16, shuffle =True)\n",
    "    \n",
    "\n",
    "\n",
    "    dataset = TensorDataset(datas, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size = batch_size, shuffle =True)\n",
    "\n",
    "    \n",
    "    progress_bar = tqdm.tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    for batch_inputs, batch_labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(dataloader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def midi_reconstruction(outputed_data):\n",
    "    \"\"\"Reconstruct the midi object from the outputed_data.\n",
    "        Arguments:\n",
    "            outputed_data: A list of outputed matrixies[batch_size * frame_number * 88 * 3]\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9559e43e",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e8a45be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youxu\\AppData\\Local\\Temp\\ipykernel_22904\\1897992020.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(actualPath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classified 32670 frames of audio, classification arcuracy 0.9295990205081112, average regression error is 0.2041722719261396\n",
      "The true accuracy of classifying pressed_key are 0.03503442496500367\n"
     ]
    }
   ],
   "source": [
    "a, b, c = validation_accuracy_check(\"test_lr_001_e_5\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5aa3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
