{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f80e4451",
   "metadata": {},
   "source": [
    "# Piano to Sheet\n",
    "\n",
    "Converting .wav piano pieces into music sheets.\n",
    "\n",
    "I\n",
    "\n",
    "## Pipeline break down ( beta)\n",
    "\n",
    "1. Loading in the wave file,\n",
    "2. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941fd603",
   "metadata": {},
   "source": [
    "## Audio Processing  Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaff679",
   "metadata": {},
   "source": [
    "For the processing libraries, we will be using pretty_midi to extract out label information from the given midi files.  \n",
    "And for processing our wav audio files. We will be using librosa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c76f67",
   "metadata": {},
   "source": [
    "## Machine Learning Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85047046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import librosa \n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pretty_midi as pm\n",
    "import mido\n",
    "import IPython.display as ipd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a26ef70",
   "metadata": {},
   "source": [
    "# Model Saving funtion and Model loading function\n",
    "\n",
    "Having a model saving function and a modle loading function in which we can train the model in small epoch progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cccab19",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ef91a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_file_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/maestro-v3.0.0.json\"\n",
    "with open(labels_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "all_sets = {}\n",
    "all_sets['train'] = []\n",
    "all_sets['validation'] = []\n",
    "all_sets['test'] = []\n",
    "\n",
    "def sortingsets (data, allsets):\n",
    "    for key in data:\n",
    "       \n",
    "        if data[key] == 'train':\n",
    "            all_sets['train'].append(key)\n",
    "        elif data[key] == 'validation':\n",
    "            all_sets['validation'].append(key)\n",
    "        else:\n",
    "            all_sets['test'].append(key)\n",
    "\n",
    "def save_index_to_csv(all_sets):\n",
    "    for key in all_sets:\n",
    "        path = f\"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/{key}_indicies.csv\"\n",
    "        df = pd.DataFrame({'Index': all_sets[key]})\n",
    "        df.to_csv(path, index=False)\n",
    "\n",
    "def load_index_from_csv(path):\n",
    "    indices = []\n",
    "    with open(path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            indices.append(int(row[0]))\n",
    "    return indices\n",
    "\n",
    "def save_progress_index_to_csv(indices, epoch):\n",
    "    path = f\"models/training_index{epoch}.csv\"\n",
    "    df = pd.DataFrame({'Index': indices})\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "def load_progress_index_from_csv(epoch):\n",
    "    path = f\"models/training_index{epoch}.csv\"\n",
    "    indices = []\n",
    "    with open(path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            indices.append(int(row[0]))\n",
    "    return indices\n",
    "\n",
    "sortingsets(data['split'], all_sets)\n",
    "save_index_to_csv(all_sets)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1870b3aa",
   "metadata": {},
   "source": [
    "traindata/maestro-v3.0.0-midi/maestro-v3.0.0/train_indicies.csv  \n",
    "traindata/maestro-v3.0.0-midi/maestro-v3.0.0/test_indicies.csv   \n",
    "traindata/maestro-v3.0.0-midi/maestro-v3.0.0/validation_indicies.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0440f3",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "implenting utility functions such as the randomizing the data set for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d594a34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### making the random seed \n",
    "np.random.seed(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77d296ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomizing the data set index for training purposes\n",
    "def randomizeing(data_set):\n",
    "    ds = np.array(data_set)\n",
    "    np.random.shuffle(ds)\n",
    "    return ds\n",
    "\n",
    "## Select n indices from the givien data set\n",
    "def select_N_randomized_from_set(n, data_set):\n",
    "    nparry = randomizeing(data_set)\n",
    "    return nparry[:n]\n",
    "\n",
    "\n",
    "## ----- ----- ---------- loading function ------------------------------ ##\n",
    "\n",
    "## function loading in the wav function\n",
    "def load_wav_from_index(index):\n",
    "    labels_file_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/maestro-v3.0.0.json\"\n",
    "    with open(labels_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    wav_path = \"traindata/maestro-v3.0.0/maestro-v3.0.0/\" + data['audio_filename'][str(index)]\n",
    "    \n",
    "    return librosa.load(wav_path, sr=None)\n",
    "\n",
    "## function loading in the midi function\n",
    "def load_midi_from_index(index):\n",
    "    labels_file_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/maestro-v3.0.0.json\"\n",
    "    with open(labels_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    midi_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/\" + data['midi_filename'][str(index)]\n",
    "    return pm.PrettyMIDI(midi_path)\n",
    "\n",
    "## ----- ----- -------- Path showing function-------------------------------- ##\n",
    "    \n",
    "## showing the file path audio of the wave\n",
    "def show_wav_path(index):\n",
    "    labels_file_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/maestro-v3.0.0.json\"\n",
    "    with open(labels_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    wav_path = \"traindata/maestro-v3.0.0/maestro-v3.0.0/\" + data['audio_filename'][str(index)]\n",
    "    return wav_path\n",
    "\n",
    "## Showing the file path of the midi file of data[index]\n",
    "def show_midi_path(index):\n",
    "    labels_file_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/maestro-v3.0.0.json\"\n",
    "    with open(labels_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    midi_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/\" + data['midi_filename'][str(index)]\n",
    "    return midi_path\n",
    "\n",
    "## loading in the wav and midi pair\n",
    "def load_wav_midi_pair(index): ## (wav, midi)\n",
    "    return load_wav_from_index(index), load_midi_from_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1db3f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_index_from_csv(\"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/train_indicies.csv\")\n",
    "print(len(train_data))\n",
    "subset_train_data = select_N_randomized_from_set(50, train_data)\n",
    "\n",
    "print(len(subset_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0651ce0a",
   "metadata": {},
   "source": [
    "# AUDIO EXPERIMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abeb97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(wav, sr), midi = load_wav_midi_pair(1025)\n",
    "# midi = load_midi_from_index(1025)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5babb367",
   "metadata": {},
   "outputs": [],
   "source": [
    "ballade1, sr = load_wav_from_index(505)\n",
    "\n",
    "noised_ballade1 = add_gaussian_noise(ballade1, noise_level=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf86bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(data= ballade1, rate= sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c3118",
   "metadata": {},
   "source": [
    "### Audio Preprocessing functions\n",
    "\n",
    "#### Pretty_midi note\n",
    "The MIDI object is used from the python package pretty_midi.  \n",
    "Using pretty_midi range from 0 to 127. we can later tranform this into the range of 0 to 87 to match a piano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1cefca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\"\"\" This function extracts all played notes in the midi Object, which it will be futher trained with the aligne ed wave object\n",
    "    input: pm object\n",
    "    It is good for debugging and seeing the midi object\n",
    "\"\"\"\n",
    "def extract_midi_notes(midi):\n",
    "    notes = []\n",
    "    for instrument in midi.instruments:\n",
    "        for note in instrument.notes:\n",
    "            notes.append({\n",
    "                'pitch': note.pitch,\n",
    "                'start': note.start,\n",
    "                'end': note.end,\n",
    "                'velocity': note.velocity\n",
    "            })\n",
    "    #preprocessing the sort\n",
    "    notes.sort(key=(lambda x: x['start']))\n",
    "    return notes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Wrting a peekable Generator for midi object\"\"\"\n",
    "class PeekableGenerator:\n",
    "    def __init__(self, generator):\n",
    "        self._generator = generator\n",
    "        self._next_item = None\n",
    "        self._has_next = False\n",
    "        self._advance()\n",
    "\n",
    "    def _advance(self):\n",
    "        try:\n",
    "            self._next_item = self._generator.__next__()\n",
    "            self._has_next = True\n",
    "        except StopIteration:\n",
    "            self._next_item = None\n",
    "            self._has_next = False\n",
    "\n",
    "    def peek(self):\n",
    "        if not self._has_next:\n",
    "            raise StopIteration(\"No more elements to peek at.\")\n",
    "        return self._next_item\n",
    "\n",
    "    def __next__(self):\n",
    "        if not self._has_next:\n",
    "            raise StopIteration(\"No more elements.\")\n",
    "        current = self._next_item\n",
    "        self._advance()\n",
    "        return current\n",
    "\n",
    "    def has_next(self):\n",
    "        return self._has_next\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield self._next_item\n",
    "        self._advance()\n",
    "\n",
    "    \n",
    "\n",
    "\"\"\"Generator to yield midi note object at the frame during the classification\n",
    "    input: pm object\n",
    "\"\"\"\n",
    "def midi_yielding(midi):\n",
    "    all_midi_obj :list = extract_midi_notes(midi)\n",
    "    ##Processing\n",
    "    for note in all_midi_obj:\n",
    "        yield note\n",
    "\n",
    "\n",
    "\"\"\" Yielding a list of midi notes information where it fits the time frame automatically.\n",
    "    Implemented using overlapping frame structure for the training.\n",
    "    Begin at 0, the frame jumping at the speed of jump_len, the size of the frame is frame_len\n",
    "    This function will yield the frame at the given parameter.\n",
    "\"\"\"\n",
    "def frame_aligning_midi(t: int, jump_len:int, frame_len: int, sr:int, midi):\n",
    "    midi_generator = PeekableGenerator(midi_yielding(midi))\n",
    "    midi_labels = []\n",
    "\n",
    "    jump_time_fraction: float = jump_len * (1/sr)\n",
    "    frame_time_fraction: float = frame_len * (1/sr)\n",
    "    print(f\"The frame jumping rate is at {jump_time_fraction} seconds, and the size of the frame is at {frame_time_fraction}.\")\n",
    "    framing = [t*jump_time_fraction, t*jump_time_fraction + frame_time_fraction]\n",
    "    \n",
    "    last_note = 0\n",
    "    while midi_generator.has_next() or last_note > framing[0]:\n",
    "        \n",
    "        while midi_generator.has_next() and midi_generator.peek()['start'] >= framing[0] and midi_generator.peek()['start'] < framing[1]:\n",
    "            try:\n",
    "                midi_labels.append(midi_generator.__next__())\n",
    "            except StopIteration:\n",
    "                print(\"Generator exhausted, no Midi Objectis being added to the label\")\n",
    "                break\n",
    "        \n",
    "        ## Yielding the list of midi notes that are fitted in side the frame\n",
    "        yield midi_labels\n",
    "\n",
    "        midi_labels.sort(key=(lambda x: x['end']))\n",
    "        ##calculating the next frame time step and removing the items from the previous frame\n",
    "        framing [0] += jump_time_fraction\n",
    "        framing [1] += jump_time_fraction\n",
    "        while len(midi_labels) > 0 and midi_labels[0]['end'] < framing[0]:\n",
    "            midi_labels.pop(0)\n",
    "        \n",
    "        if len(midi_labels) > 0 and midi_labels[0]['end'] > framing[0]:\n",
    "            last_note = midi_labels[0]['end']\n",
    "\n",
    "    \n",
    "        \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\" This function returns the aligned frame at the wav data,\"\"\"\n",
    "def frame_aligning_wav(t: int, jump_len: int, frame_len: int, wav):\n",
    "    begin = t * jump_len\n",
    "    return wav[begin: begin + frame_len]\n",
    "\n",
    "\n",
    "\"\"\" This function returns the amount seconds of audio data from the wav, began on t, while using  \"\"\"\n",
    "\n",
    "def audio_segment_of(t: int, wav, seconds: float, sr: int, jump_len: int = 512, frame_len: int = 2048, ):\n",
    "    size = int(seconds*sr)\n",
    "    begin = t*jump_len\n",
    "    \n",
    "    return wav[begin: begin + size]\n",
    "\n",
    "def audio_segment_between(begin, end, wav, sr):\n",
    "    return wav[int(begin*sr): int(end*sr)]\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260c0e13",
   "metadata": {},
   "source": [
    "### Constructing Mel Spectrogram From wav frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7622f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ballade1, sr), midi = load_wav_midi_pair(505)\n",
    "\n",
    "for i in frame_aligning_midi(0, 512, 2048, sr, midi):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd542b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ballade1, sr = load_wav_from_index(505)\n",
    "audio = audio_segment_of(0, ballade1, 3, sr)\n",
    "audio1 = audio_segment_between(500,501,ballade1, sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8986a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(data= audio1, rate= sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8191371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mel_spectrogram(audio, sr, n_mels=88, hop_length=512, n_fft=2048): ##_fft is the frame length \n",
    "    \"\"\"\n",
    "    Extract a mel-spectrogram from raw audio.\n",
    "    \"\"\"\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio, sr=sr, n_mels=n_mels, hop_length=hop_length, n_fft=n_fft\n",
    "    )\n",
    "    # Convert to log scale\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return mel_spec_db\n",
    "\n",
    "# Example audio input\n",
    "mel_spectrogram = extract_mel_spectrogram(audio1, sr)\n",
    "\n",
    "# Normalize the spectrogram for display\n",
    "mel_spectrogram_normalized = (mel_spectrogram - mel_spectrogram.min()) / (mel_spectrogram.max() - mel_spectrogram.min())\n",
    "\n",
    "# Plot the mel-spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(mel_spectrogram_normalized, sr=sr, x_axis='time', y_axis='mel')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel-Spectrogram')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b5e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d73318",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e9eb5b8",
   "metadata": {},
   "source": [
    "### Calculating the added noise level to the wav function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea68b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_noise_power(audio, snr_dB):\n",
    "    \"\"\"\n",
    "    Compute the noise power needed for a given SNR in dB.\n",
    "    :param audio: NumPy array of the audio signal.\n",
    "    :param snr_dB: Desired Signal-to-Noise Ratio in dB.\n",
    "    :return: Noise power.\n",
    "    \"\"\"\n",
    "    # Calculate signal power (mean squared amplitude)\n",
    "    signal_power = np.mean(audio ** 2)\n",
    "    \n",
    "    # Convert SNR from dB to linear scale\n",
    "    snr_linear = 10 ** (snr_dB / 10)\n",
    "    \n",
    "    # Calculate noise power\n",
    "    noise_power = signal_power / snr_linear\n",
    "    return noise_power\n",
    "\n",
    "div = 100\n",
    "somewav = select_N_randomized_from_set(div, train_data)\n",
    "print(len(somewav))\n",
    "summing = 0 #summing the proper noise level\n",
    "for i in range(10):\n",
    "    wav, _ = load_wav_from_index(somewav[i])\n",
    "    summing += np.sqrt(compute_noise_power(wav, 20)) ## desired training SNR to be 15 dB\n",
    "\n",
    "print(f\"Our desired training noise is about {summing/div}, setting it to be our default added noise to wav\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c50b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(audio, noise_level=0.0006):\n",
    "    \n",
    "    noise = np.random.normal(0, noise_level, audio.shape)\n",
    "    return audio + noise\n",
    "\n",
    "def framelining (time, jump, sr):\n",
    "    \n",
    "    frametime = jump/sr\n",
    "    print(22*frametime)\n",
    "\n",
    "framelining(1, 2048, 44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c59f752",
   "metadata": {},
   "source": [
    "### Model for musheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a81def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PianoNoteModel(nn.Module):\n",
    "    def __init__(self, num_mel_bins=88, num_frames=89, output_size=(88, 3)):\n",
    "        super(PianoNoteModel, self).__init__()\n",
    "        \n",
    "        # Convolution Layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Pooling layres\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Drop out layers\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        # Flatten size estimation (you may need to adjust this for your input shape)\n",
    "        flattened_size = (num_mel_bins // 2) * (num_frames // 2) * 64\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(flattened_size, 512)\n",
    "        self.fc2 = nn.Linear(512, output_size[0] * output_size[1])  # 88 pitches * 3 outputs per pitch\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, 1, num_mel_bins, num_frames)\n",
    "        \n",
    "        # Convolutional layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten except batch dimension\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Reshape to output dimensions: (batch_size, num_frames, 88, 3)\n",
    "        x = x.view(x.size(0), -1, 88, 3)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "# Custom loss function\n",
    "class MultiTaskLoss(nn.Module):\n",
    "    def __init__(self, classification_weight=1.0, regression_weight=1.0):\n",
    "        super(MultiTaskLoss, self).__init__()\n",
    "        self.classification_loss = nn.CrossEntropyLoss()\n",
    "        self.regression_loss = nn.MSELoss()\n",
    "        self.classification_weight = classification_weight\n",
    "        self.regression_weight = regression_weight\n",
    "\n",
    "    def forward(self, classification_output, classification_target, \n",
    "                regression_output, regression_target):\n",
    "        # Compute classification loss\n",
    "        class_loss = self.classification_loss(classification_output, classification_target)\n",
    "        \n",
    "        # Compute regression loss\n",
    "        reg_loss = self.regression_loss(regression_output, regression_target)\n",
    "        \n",
    "        # Combine with weights\n",
    "        total_loss = self.classification_weight * class_loss + self.regression_weight * reg_loss\n",
    "        \n",
    "        return total_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04663d46",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78ededb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "## MODEL Saver\n",
    "def save_model(model, optimizer, time, path = \"model/\"):\n",
    "    actualPath = path + f\"{time}_piano_model.pth\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, actualPath)\n",
    "    print(f\"Model saved to {actualPath}\")\n",
    "\n",
    "## MODEL loader\n",
    "def load_model(model, optimizer, path):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "\"\"\"This function generates the matrix label to aligne the mal_spectral gram\n",
    "each matrix should consired num_frame,\n",
    "poping fram_jumping after the matrix is being yield\n",
    "\n",
    "jumping time should be\n",
    "frame_jumping * (frame_len /sr)\n",
    "\"\"\"\n",
    "def label_generator(num_frame, frame_jumping, jump_len:int, frame_len: int, sr:int, midi):\n",
    "    midi_frame_gen = PeekableGenerator(frame_aligning_midi(0, frame_len, frame_len, sr, midi)) ## Note, while using the architechture of mel_spectrogram, the we don't need to consider the jump offset in the midi side\n",
    "    label = []\n",
    "    counter = 0\n",
    "    \n",
    "    \n",
    "    concur_time = 0\n",
    "    concur_time_end = 0\n",
    "    frame_time = frame_len /sr\n",
    "\n",
    "    \n",
    "    while midi_frame_gen.has_next():\n",
    "        if (len(label) < num_frame):\n",
    "            label.append(list(midi_frame_gen.__next__()))\n",
    "            concur_time_end += frame_time\n",
    "            \n",
    "        else:\n",
    "            counter+=1\n",
    "            yield label\n",
    "            for i in range(int(frame_jumping)):\n",
    "                label.pop(0)\n",
    "                concur_time += frame_time\n",
    "                \n",
    "\n",
    "\n",
    "\"\"\"This function tries to mimic the decayed velocity miniking the sound at which a piano has been decayed\"\"\"\n",
    "def velocity_decay_sustain (velocity, onset, at_time):\n",
    "    if (at_time - onset < 0.2):\n",
    "        return velocity\n",
    "    else:\n",
    "        return math.exp((onset - at_time) * 0.6) * velocity\n",
    "\n",
    "\n",
    "\"\"\"formating the label of list, in to a matrix of 88 * 3 matrix.\n",
    "    each row represent a strikable key, \n",
    "    column 1 (being stricked) : 0, 1 (classification purpose)\n",
    "    column 2 (onset timer) : the set of positive interger that is less than onset. (Regression purpose)\n",
    "    column 3 (velocity of which is being stricked) : the set of positive integer that is less than onset. (Regression purpose) \n",
    "        note for the velocity of the piano key will be approximately alingned with a decay parameter\"\"\"\n",
    "def label_formater(label, frameonset):\n",
    "    ret_label = np.zeros((len(label), 88,3))\n",
    "\n",
    "    for i in range(len(label)) :\n",
    "        for key_obj in label[i]:\n",
    "            pitch = key_obj['pitch'] - 20\n",
    "            ret_label[i][pitch][0] = 1\n",
    "            ret_label[i][pitch][1] = key_obj['start'] - frameonset if key_obj['start'] > frameonset else 0\n",
    "            ret_label[i][pitch][2] = velocity_decay_sustain(key_obj['velocity'],  key_obj['start'], frameonset) \n",
    "\n",
    "    return ret_label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"This function assure the input for the training will retain the dimension in the case of track is ending.\"\"\"\n",
    "def construct_input(spectrogram, x, y):\n",
    "    if spectrogram.shape == (x,y):\n",
    "        return spectrogram\n",
    "    else: \n",
    "        ret = np.zeros((x,y))\n",
    "        for i in range(spectrogram.shape[0]):\n",
    "            ret[i] = spectrogram[i]\n",
    "        return ret\n",
    "\n",
    "def train_segment(wav, sr, length=1, hop=0.5):\n",
    "    ret = []\n",
    "    max = len(wav)/sr\n",
    "    begin = 0\n",
    "    while (begin < max):\n",
    "        adding = length if begin + length < max else max - begin\n",
    "        ret.append((begin, begin + adding))\n",
    "        begin += hop\n",
    "\n",
    "    return ret\n",
    "    \n",
    "\n",
    "def label_buffer(num_frame, label):\n",
    "    if len(label) < num_frame:\n",
    "        for i in range(num_frame- len(label)):\n",
    "            label.append([])\n",
    "    return label\n",
    "\n",
    "\n",
    "\"\"\" This function will generate all the mel_spectrogram and label pair for each audio segment of the song at index\"\"\"\n",
    "def generate_data_label(index, num_frame=22, segment_jump = 0.5, frame_length = 2048, hop_length = 512):\n",
    "    # fetching infomation\n",
    "    (wav, sr), midi = load_wav_midi_pair(index)\n",
    "    \n",
    "    frame_time =  frame_length/sr\n",
    "    quick_sample = audio_segment_between(0, num_frame*frame_time, wav, sr)\n",
    "    x, y = extract_mel_spectrogram(quick_sample, sr).shape\n",
    "    segments = train_segment(wav, sr, length = num_frame * frame_time, hop= num_frame * frame_time/2)\n",
    "    \n",
    "    label_gen = PeekableGenerator(label_generator(num_frame, num_frame*segment_jump, hop_length, frame_length, sr, midi))\n",
    "    \n",
    "    input_datas = []\n",
    "    labels = []\n",
    "    for beg, end in segments:\n",
    "        \n",
    "        if (label_gen.has_next() is False) :\n",
    "            break\n",
    "\n",
    "        audio = audio_segment_between(beg, end, wav, sr)\n",
    "        mel_spectrogram = extract_mel_spectrogram(audio, sr, hop_length= hop_length, n_fft=frame_length)\n",
    "        input_data = construct_input(mel_spectrogram, x,y)\n",
    "        input_data = np.array(input_data)\n",
    "        input_data = np.expand_dims(input_data, axis=0)\n",
    "\n",
    "        label = label_buffer(num_frame, label_gen.__next__())\n",
    "        label = label_formater(label, beg)\n",
    "        label = np.array(label)\n",
    "        \n",
    "\n",
    "        input_datas.append(input_data)\n",
    "        print(label.shape)\n",
    "        labels.append(label)\n",
    "        \n",
    "    return np.array(input_datas), np.array(labels)\n",
    "   \n",
    "\n",
    "\n",
    "\"\"\"The training is based on how many frame should be trained at a time,\n",
    "    the default setting is suited for the expriment set up above,\n",
    "    num_frame is tried to aligned it to approx 1 second of the sample\n",
    "    segment_jumping would be trying to get 50% of the sample audio\n",
    "\"\"\"\n",
    "def one_pass_song_train(model_path: str, version: str, optimizer, criterion, index, num_frame=22, segment_jump = 0.5, frame_length = 2048, hop_length = 512):\n",
    "    #load model\n",
    "    model = PianoNoteModel()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    criterion = nn.MSELoss()\n",
    " \n",
    "    if model_path is not None:\n",
    "        model, optimizer = load_model(model, optimizer)\n",
    "\n",
    "    # generate all data\n",
    "    datas, labels = generate_data_label(index, num_frame=num_frame, segment_jump=segment_jump, frame_length=frame_length, hop_length=hop_length)\n",
    "    datas = torch.tensor(datas, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    dataset = TensorDataset(datas, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size = 16, shuffle =True)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    lossval = 0.0\n",
    "    for batch_inputs, batch_labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_inputs)\n",
    "\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        lossval+= loss.item()\n",
    "        \n",
    "    #save model\n",
    "    save_model(model, optimizer, time.time)\n",
    "    return lossval\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_N_song_on_epoch (epoch, N):\n",
    "    \n",
    "\n",
    "    tobe_done = load_progress_index_from_csv(epoch)\n",
    "\n",
    "    ## indices to be trained for the current run\n",
    "    parse_in = [tobe_done.pop() for i in range(N)]\n",
    "\n",
    "    \n",
    "    ## Passing in each index to train on one_pass_song_train\n",
    "    for index in parse_in:\n",
    "        one_pass_song_train()\n",
    "\n",
    "    \n",
    "    print(\"Message from train_N_song_on_epoch\") \n",
    "    if (len(tobe_done) == 0):\n",
    "        print(f\"Epoch {epoch} training complete\")\n",
    "    else :\n",
    "        \n",
    "        print(f\"Epoch {epoch} have trained {N} songs, left over songs for current epoch will be saved. Need to train {len(tobe_done)}.\")\n",
    "        save_progress_index_to_csv(tobe_done, epoch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d5066c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b42ac20",
   "metadata": {},
   "source": [
    "### Index Processing\n",
    "\n",
    "Using the saved indices we have processed before  \n",
    "Here are the file paths  \n",
    "for training datas :traindata/maestro-v3.0.0-midi/maestro-v3.0.0/train_indicies.csv  \n",
    "for test datas :traindata/maestro-v3.0.0-midi/maestro-v3.0.0/test_indicies.csv   \n",
    "for validataion datas :traindata/maestro-v3.0.0-midi/maestro-v3.0.0/validation_indicies.csv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64563c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only run this once\n",
    "train_indices = load_index_from_csv(\"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/train_indicies.csv\")\n",
    "validation_indices = load_index_from_csv(\"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/validation_indicies.csv\")\n",
    "test_indices = load_index_from_csv(\"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/test_indicies.csv\")\n",
    "\n",
    "train_indices = randomizeing(train_indices)\n",
    "save_progress_index_to_csv(train_indices, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc9808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 have trained 50 songs, left over songs for current epoch will be saved. Need to train 912\n"
     ]
    }
   ],
   "source": [
    "pushblish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da5c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
