{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f80e4451",
   "metadata": {},
   "source": [
    "# Piano to Sheet\n",
    "\n",
    "Converting .wav piano pieces into music sheets.\n",
    "\n",
    "I\n",
    "\n",
    "## Pipeline break down ( beta)\n",
    "\n",
    "1. Loading in the wave file,\n",
    "2. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941fd603",
   "metadata": {},
   "source": [
    "## Audio Processing  Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaff679",
   "metadata": {},
   "source": [
    "For the processing libraries, we will be using pretty_midi to extract out label information from the given midi files.  \n",
    "And for processing our wav audio files. We will be using librosa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c76f67",
   "metadata": {},
   "source": [
    "## Machine Learning Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85047046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import librosa \n",
    "import json\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pretty_midi as pm\n",
    "import mido\n",
    "import IPython.display as ipd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a26ef70",
   "metadata": {},
   "source": [
    "# Model Saving funtion and Model loading function\n",
    "\n",
    "Having a model saving function and a modle loading function in which we can train the model in small epoch progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cccab19",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ef91a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_file_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/maestro-v3.0.0.json\"\n",
    "with open(labels_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "all_sets = {}\n",
    "all_sets['train'] = []\n",
    "all_sets['validation'] = []\n",
    "all_sets['test'] = []\n",
    "\n",
    "def sortingsets (data, allsets):\n",
    "    for key in data:\n",
    "       \n",
    "        if data[key] == 'train':\n",
    "            all_sets['train'].append(key)\n",
    "        elif data[key] == 'validation':\n",
    "            all_sets['validation'].append(key)\n",
    "        else:\n",
    "            all_sets['test'].append(key)\n",
    "\n",
    "def save_index_to_csv(all_sets):\n",
    "    for key in all_sets:\n",
    "        path = f\"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/{key}_indicies.csv\"\n",
    "        df = pd.DataFrame({'Index': all_sets[key]})\n",
    "        df.to_csv(path, index=False)\n",
    "\n",
    "def load_index_from_csv(path):\n",
    "    indices = []\n",
    "    with open(path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            indices.append(int(row[0]))\n",
    "    return indices\n",
    "\n",
    "def save_progress_index_to_csv(indices, epoch, id):\n",
    "    path = f\"models/training_index{epoch}_for_{id}.csv\"\n",
    "    df = pd.DataFrame({'Index': indices})\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "def load_progress_index_from_csv(epoch, id):\n",
    "    path = f\"models/training_index{epoch}_for_{id}.csv\"\n",
    "    indices = []\n",
    "    with open(path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            indices.append(int(row[0]))\n",
    "    return indices\n",
    "\n",
    "sortingsets(data['split'], all_sets)\n",
    "save_index_to_csv(all_sets)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1870b3aa",
   "metadata": {},
   "source": [
    "traindata/maestro-v3.0.0-midi/maestro-v3.0.0/train_indicies.csv  \n",
    "traindata/maestro-v3.0.0-midi/maestro-v3.0.0/test_indicies.csv   \n",
    "traindata/maestro-v3.0.0-midi/maestro-v3.0.0/validation_indicies.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0440f3",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "implenting utility functions such as the randomizing the data set for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d594a34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### making the random seed \n",
    "np.random.seed(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77d296ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomizing the data set index for training purposes\n",
    "def randomizeing(data_set):\n",
    "    ds = np.array(data_set)\n",
    "    np.random.shuffle(ds)\n",
    "    return ds\n",
    "\n",
    "## Select n indices from the givien data set\n",
    "def select_N_randomized_from_set(n, data_set):\n",
    "    nparry = randomizeing(data_set)\n",
    "    return nparry[:n]\n",
    "\n",
    "\n",
    "## ----- ----- ---------- loading function ------------------------------ ##\n",
    "\n",
    "## function loading in the wav function\n",
    "def load_wav_from_index(index):\n",
    "    labels_file_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/maestro-v3.0.0.json\"\n",
    "    with open(labels_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    wav_path = \"traindata/maestro-v3.0.0/maestro-v3.0.0/\" + data['audio_filename'][str(index)]\n",
    "    \n",
    "    return librosa.load(wav_path, sr=None)\n",
    "\n",
    "## function loading in the midi function\n",
    "def load_midi_from_index(index):\n",
    "    labels_file_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/maestro-v3.0.0.json\"\n",
    "    with open(labels_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    midi_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/\" + data['midi_filename'][str(index)]\n",
    "    return pm.PrettyMIDI(midi_path)\n",
    "\n",
    "## ----- ----- -------- Path showing function-------------------------------- ##\n",
    "    \n",
    "## showing the file path audio of the wave\n",
    "def show_wav_path(index):\n",
    "    labels_file_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/maestro-v3.0.0.json\"\n",
    "    with open(labels_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    wav_path = \"traindata/maestro-v3.0.0/maestro-v3.0.0/\" + data['audio_filename'][str(index)]\n",
    "    return wav_path\n",
    "\n",
    "## Showing the file path of the midi file of data[index]\n",
    "def show_midi_path(index):\n",
    "    labels_file_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/maestro-v3.0.0.json\"\n",
    "    with open(labels_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    midi_path = \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/\" + data['midi_filename'][str(index)]\n",
    "    return midi_path\n",
    "\n",
    "## loading in the wav and midi pair\n",
    "def load_wav_midi_pair(index): ## (wav, midi)\n",
    "    return load_wav_from_index(index), load_midi_from_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1db3f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_index_from_csv(\"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/train_indicies.csv\")\n",
    "print(len(train_data))\n",
    "subset_train_data = select_N_randomized_from_set(50, train_data)\n",
    "\n",
    "print(len(subset_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0651ce0a",
   "metadata": {},
   "source": [
    "# AUDIO EXPERIMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abeb97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(wav, sr), midi = load_wav_midi_pair(1025)\n",
    "# midi = load_midi_from_index(1025)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5babb367",
   "metadata": {},
   "outputs": [],
   "source": [
    "ballade1, sr = load_wav_from_index(505)\n",
    "\n",
    "noised_ballade1 = add_gaussian_noise(ballade1, noise_level=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d34459",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ballade1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fdace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(noised_ballade1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf86bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, sr = load_wav_from_index(216)\n",
    "\n",
    "\n",
    "ipd.Audio(data= wav, rate= sr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c3118",
   "metadata": {},
   "source": [
    "### Audio Preprocessing functions\n",
    "\n",
    "#### Pretty_midi note\n",
    "The MIDI object is used from the python package pretty_midi.  \n",
    "Using pretty_midi range from 0 to 127. we can later tranform this into the range of 0 to 87 to match a piano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc1cefca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\"\"\" This function extracts all played notes in the midi Object, which it will be futher trained with the aligne ed wave object\n",
    "    input: pm object\n",
    "    It is good for debugging and seeing the midi object\n",
    "\"\"\"\n",
    "def extract_midi_notes(midi):\n",
    "    notes = []\n",
    "    for instrument in midi.instruments:\n",
    "        for note in instrument.notes:\n",
    "            notes.append({\n",
    "                'pitch': note.pitch,\n",
    "                'start': note.start,\n",
    "                'end': note.end,\n",
    "                'velocity': note.velocity\n",
    "            })\n",
    "    #preprocessing the sort\n",
    "    notes.sort(key=(lambda x: x['start']))\n",
    "    return notes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Wrting a peekable Generator for midi object\"\"\"\n",
    "class PeekableGenerator:\n",
    "    def __init__(self, generator):\n",
    "        self._generator = generator\n",
    "        self._next_item = None\n",
    "        self._has_next = False\n",
    "        self._advance()\n",
    "\n",
    "    def _advance(self):\n",
    "        try:\n",
    "            self._next_item = self._generator.__next__()\n",
    "            self._has_next = True\n",
    "        except StopIteration:\n",
    "            self._next_item = None\n",
    "            self._has_next = False\n",
    "\n",
    "    def peek(self):\n",
    "        if not self._has_next:\n",
    "            raise StopIteration(\"No more elements to peek at.\")\n",
    "        return self._next_item\n",
    "\n",
    "    def __next__(self):\n",
    "        if not self._has_next:\n",
    "            raise StopIteration(\"No more elements.\")\n",
    "        current = self._next_item\n",
    "        self._advance()\n",
    "        return current\n",
    "\n",
    "    def has_next(self):\n",
    "        return self._has_next\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield self._next_item\n",
    "        self._advance()\n",
    "\n",
    "    \n",
    "\n",
    "\"\"\"Generator to yield midi note object at the frame during the classification\n",
    "    input: pm object\n",
    "\"\"\"\n",
    "def midi_yielding(midi):\n",
    "    all_midi_obj :list = extract_midi_notes(midi)\n",
    "    ##Processing\n",
    "    for note in all_midi_obj:\n",
    "        yield note\n",
    "\n",
    "\n",
    "\"\"\" Yielding a list of midi notes information where it fits the time frame automatically.\n",
    "    Implemented using overlapping frame structure for the training.\n",
    "    Begin at 0, the frame jumping at the speed of jump_len, the size of the frame is frame_len\n",
    "    This function will yield the frame at the given parameter.\n",
    "\"\"\"\n",
    "def frame_aligning_midi(t: int, jump_len:int, frame_len: int, sr:int, midi):\n",
    "    midi_generator = PeekableGenerator(midi_yielding(midi))\n",
    "    midi_labels = []\n",
    "\n",
    "    jump_time_fraction: float = jump_len * (1/sr)\n",
    "    frame_time_fraction: float = frame_len * (1/sr)\n",
    "    framing = [t*jump_time_fraction, t*jump_time_fraction + frame_time_fraction]\n",
    "    \n",
    "    last_note = 0\n",
    "    while midi_generator.has_next() or last_note > framing[0]:\n",
    "        \n",
    "        while midi_generator.has_next() and midi_generator.peek()['start'] >= framing[0] and midi_generator.peek()['start'] < framing[1]:\n",
    "            try:\n",
    "                midi_labels.append(midi_generator.__next__())\n",
    "            except StopIteration:\n",
    "                print(\"Generator exhausted, no Midi Objectis being added to the label\")\n",
    "                break\n",
    "        \n",
    "        ## Yielding the list of midi notes that are fitted in side the frame\n",
    "        yield midi_labels\n",
    "\n",
    "        midi_labels.sort(key=(lambda x: x['end']))\n",
    "        ##calculating the next frame time step and removing the items from the previous frame\n",
    "        framing [0] += jump_time_fraction\n",
    "        framing [1] += jump_time_fraction\n",
    "        while len(midi_labels) > 0 and midi_labels[0]['end'] < framing[0]:\n",
    "            midi_labels.pop(0)\n",
    "        \n",
    "        if len(midi_labels) > 0 and midi_labels[0]['end'] > framing[0]:\n",
    "            last_note = midi_labels[0]['end']\n",
    "\n",
    "    \n",
    "        \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\" This function returns the aligned frame at the wav data,\"\"\"\n",
    "def frame_aligning_wav(t: int, jump_len: int, frame_len: int, wav):\n",
    "    begin = t * jump_len\n",
    "    return wav[begin: begin + frame_len]\n",
    "\n",
    "\n",
    "\"\"\" This function returns the amount seconds of audio data from the wav, began on t, while using  \"\"\"\n",
    "\n",
    "def audio_segment_of(t: int, wav, seconds: float, sr: int, jump_len: int = 512, frame_len: int = 2048, ):\n",
    "    size = int(seconds*sr)\n",
    "    begin = t*jump_len\n",
    "    \n",
    "    return wav[begin: begin + size]\n",
    "\n",
    "def audio_segment_between(begin, end, wav, sr):\n",
    "    return wav[int(begin*sr): int(end*sr)]\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260c0e13",
   "metadata": {},
   "source": [
    "### Constructing Mel Spectrogram From wav frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8191371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mel_spectrogram(audio, sr, n_mels=88, hop_length=512, n_fft=4096): ##_fft is the frame length \n",
    "    \"\"\"\n",
    "    Extract a mel-spectrogram from raw audio.\n",
    "    \"\"\"\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio, sr=sr, n_mels=n_mels, hop_length=hop_length, n_fft=n_fft\n",
    "    )\n",
    "    # Convert to log scale\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return mel_spec_db\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b5e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example audio input\n",
    "frame_time = 2048 /sr\n",
    "num_frame = 22\n",
    "audio1 = audio_segment_between(0,2,ballade1, sr)\n",
    "quick_sample = audio_segment_between(0, num_frame*frame_time, ballade1, sr)\n",
    "mel_spectrogram = extract_mel_spectrogram(audio1, sr, n_mels =188)\n",
    "print(mel_spectrogram.shape)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "librosa.display.specshow(mel_spectrogram, \n",
    "                         sr=sr, \n",
    "                         x_axis=\"linear\")\n",
    "plt.colorbar(format=\"%+2.f\")\n",
    "plt.show()\n",
    "ipd.Audio(data= audio1, rate= sr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d73318",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e9eb5b8",
   "metadata": {},
   "source": [
    "### Calculating the added noise level to the wav function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10ea68b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_noise_power(audio, snr_dB):\n",
    "    \"\"\"\n",
    "    Compute the noise power needed for a given SNR in dB.\n",
    "    :param audio: NumPy array of the audio signal.\n",
    "    :param snr_dB: Desired Signal-to-Noise Ratio in dB.\n",
    "    :return: Noise power.\n",
    "    \"\"\"\n",
    "    # Calculate signal power (mean squared amplitude)\n",
    "    signal_power = np.mean(audio ** 2)\n",
    "    \n",
    "    # Convert SNR from dB to linear scale\n",
    "    snr_linear = 10 ** (snr_dB / 10)\n",
    "    \n",
    "    # Calculate noise power\n",
    "    noise_power = signal_power / snr_linear\n",
    "    return noise_power\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24c50b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(audio, noise_level=0.0006):\n",
    "    \n",
    "    noise = np.random.normal(0, noise_level, audio.shape)\n",
    "    return audio + noise\n",
    "\n",
    "def framelining (time, jump, sr):\n",
    "    \n",
    "    frametime = jump/sr\n",
    "    print(22*frametime)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c59f752",
   "metadata": {},
   "source": [
    "### Model for musheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6a81def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PianoNoteModel(nn.Module):\n",
    "    def __init__(self, num_mel_bins=88, mel_temporal_length=89, num_frame_output=22, output_size=(88, 3)):\n",
    "        \"\"\"The default parameter is approximated for 1 seconds of audio data, regarding to the temporal_length\"\"\"\n",
    "        super(PianoNoteModel, self).__init__()\n",
    "\n",
    "        self.num_mel_bins = num_mel_bins\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        # Compute flattened size based on input dimensions after pooling\n",
    "        # Assuming input shape is (batch_size, 1, num_mel_bins, num_frames)\n",
    "        pooled_mel_bins = num_mel_bins // 2  # Adjust based on pooling\n",
    "        pooled_temporal_length = mel_temporal_length // 2     # Adjust based on pooling\n",
    "\n",
    "        \n",
    "        flattened_size = pooled_mel_bins * pooled_temporal_length * 64  # Based on conv2 output channels\n",
    "        \n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(flattened_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_frame_output * output_size[0] * output_size[1])  # Predict for each frame\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, 1, num_mel_bins, num_frames)\n",
    "        \n",
    "        # Convolutional layers\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "\n",
    "       \n",
    "\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten except batch dimension\n",
    "       \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "       \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Reshape to output dimensions: (batch_size, num_frames, 88, 3)\n",
    "        x = x.view(x.size(0), -1, 88, 3)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "# Custom loss function, for mutipurpose loss function in the output layer\n",
    "class MultiTaskLoss(nn.Module):\n",
    "    def __init__(self, classification_weight=1.0, regression_weight=1.0):\n",
    "        super(MultiTaskLoss, self).__init__()\n",
    "        self.classification_loss = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        self.regression_loss = nn.MSELoss()\n",
    "        self.classification_weight = classification_weight\n",
    "        self.regression_weight = regression_weight\n",
    "\n",
    "    def forward(self, classification_output, classification_target, \n",
    "                regression_output, regression_target, ratio):\n",
    "        # Compute classification loss\n",
    "        loss_per_element  = self.classification_loss(classification_output, classification_target)\n",
    "\n",
    "        pressed_key_mask = classification_target == 1\n",
    "        unpressed_key_mask = classification_target == 0\n",
    "\n",
    "        pressed_key_label = classification_target[pressed_key_mask]\n",
    "        unpressed_key_label = classification_target[unpressed_key_mask]\n",
    "\n",
    "\n",
    "        pressed_keynumber = pressed_key_label.numel()\n",
    "        unpressed_keynumber = unpressed_key_label.numel()\n",
    "\n",
    "\n",
    "        # Compute the scoring of pressedkey and unpressed key with the feedback\n",
    "        epsilon = 1e-8  # Small constant to avoid instability\n",
    "        if (pressed_keynumber == 0):\n",
    "            w_pressed = 0\n",
    "        else: \n",
    "            w_pressed = ((unpressed_keynumber/(pressed_keynumber+ epsilon))**(ratio/2)) #* feed_back[0]\n",
    "            \n",
    "        if (unpressed_keynumber == 0):\n",
    "            w_unpressed = 0\n",
    "        else: \n",
    "            w_unpressed= ((pressed_keynumber/(unpressed_keynumber+ epsilon))**(ratio/2)) #* feed_back[1]\n",
    "        #total =  pressed_keynumber + unpressed_keynumber\n",
    "        #w_pressed = unpressed_keynumber /total\n",
    "        #w_unpressed = pressed_keynumber / total\n",
    "\n",
    "        weight_matrix = torch.full_like(classification_target, w_unpressed)\n",
    "        weight_matrix[pressed_key_mask] = w_pressed\n",
    "        \n",
    "        weighted_class_loss_mat = loss_per_element * weight_matrix\n",
    "        \n",
    "        weighted_class_loss = weighted_class_loss_mat.sum()\n",
    "        \n",
    "        \n",
    "        \"\"\"# Computing the current accuracy of pressed and non pressed keys\n",
    "        predicted_class = (classification_output > 0.5).float()\n",
    "        correct_pressed = (predicted_class[pressed_key_mask] == classification_target[pressed_key_mask]).sum().item()\n",
    "        correct_unpressed = (predicted_class[unpressed_key_mask] == classification_target[unpressed_key_mask]).sum().item()\n",
    "        total_pressed = pressed_key_mask.sum().item()\n",
    "        total_unpressed = unpressed_key_mask.sum().item()\n",
    "\n",
    "\n",
    "\n",
    "        # update the feedback based on the accuracy\n",
    "        epsilon = 1e-6  # Small constant to avoid instability\n",
    "        correct_pressed_ratio = max(correct_pressed / total_pressed, epsilon)\n",
    "        correct_unpressed_ratio = max(correct_unpressed / total_unpressed, epsilon)\n",
    "\n",
    "        feed_back[0] = 1 / correct_pressed_ratio\n",
    "        feed_back[1] = 1 / correct_unpressed_ratio\"\"\"\n",
    "\n",
    "        # Compute regression loss\n",
    "        reg_loss = self.regression_loss(regression_output, regression_target)\n",
    "        \n",
    "        # Combine with weights\n",
    "        total_loss = self.classification_weight * weighted_class_loss + self.regression_weight * reg_loss\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def set_classification_balancer(self, weight):\n",
    "        self.classification_loss.weight = weight\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04663d46",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b78ededb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "\n",
    "## MODEL Saver\n",
    "def save_model(model, optimizer, id, path = \"models/\"):\n",
    "    \"\"\" Saving the model after each training/testing before each training progress\"\"\"\n",
    "    actualPath = path + f\"{id}_piano_model.pth\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, actualPath)\n",
    "    \n",
    "\n",
    "## MODEL loader\n",
    "def load_model(model, optimizer, id, path = \"models/\"):\n",
    "    actualPath = path + f\"{id}_piano_model.pth\"\n",
    "    checkpoint = torch.load(actualPath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "\"\"\"This function generates the matrix label to aligne the mal_spectral gram\n",
    "each matrix should consired num_frame,\n",
    "poping fram_jumping after the matrix is being yield\n",
    "\n",
    "jumping time should be\n",
    "frame_jumping * (frame_len /sr)\n",
    "\"\"\"\n",
    "def label_generator(num_frame, frame_jumping, jump_len:int, frame_len: int, sr:int, midi):\n",
    "    midi_frame_gen = PeekableGenerator(frame_aligning_midi(0, frame_len, frame_len, sr, midi)) ## Note, while using the architechture of mel_spectrogram, the we don't need to consider the jump offset in the midi side\n",
    "    label = []\n",
    "    counter = 0\n",
    "    \n",
    "    \n",
    "    concur_time = 0\n",
    "    concur_time_end = 0\n",
    "    frame_time = frame_len /sr\n",
    "\n",
    "    \n",
    "    while midi_frame_gen.has_next():\n",
    "        if (len(label) < num_frame):\n",
    "            label.append(list(midi_frame_gen.__next__()))\n",
    "            concur_time_end += frame_time\n",
    "            \n",
    "        else:\n",
    "            counter+=1\n",
    "            yield label\n",
    "            for i in range(int(frame_jumping)):\n",
    "                label.pop(0)\n",
    "                concur_time += frame_time\n",
    "                \n",
    "\n",
    "\n",
    "\"\"\"This function tries to mimic the decayed velocity miniking the sound at which a piano has been decayed\"\"\"\n",
    "def velocity_decay_sustain (velocity, onset, at_time):\n",
    "    if (at_time - onset < 0.2):\n",
    "        return velocity\n",
    "    else:\n",
    "        return math.exp((onset - at_time) * 0.6) * velocity\n",
    "\n",
    "\n",
    "\"\"\"formating the label of list, in to a matrix of 88 * 3 matrix.\n",
    "    each row represent a strikable key, \n",
    "    column 1 (being stricked) : 0, 1 (classification purpose)\n",
    "    column 2 (onset timer) : the set of positive interger that is less than onset. (Regression purpose)\n",
    "    column 3 (velocity of which is being stricked) : the set of positive integer that is less than onset. (Regression purpose) \n",
    "        note for the velocity of the piano key will be approximately alingned with a decay parameter\"\"\"\n",
    "def label_formater(label, frameonset):\n",
    "    ret_label = np.zeros((len(label), 88,3))\n",
    "\n",
    "    for i in range(len(label)) :\n",
    "        for key_obj in label[i]:\n",
    "            pitch = key_obj['pitch'] - 21\n",
    "            ret_label[i][pitch][0] = 1\n",
    "            ret_label[i][pitch][1] = key_obj['start'] - frameonset if key_obj['start'] > frameonset else 0\n",
    "            ret_label[i][pitch][2] = velocity_decay_sustain(key_obj['velocity'],  key_obj['start'], frameonset) \n",
    "\n",
    "    return ret_label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"This function assure the input for the training will retain the dimension in the case of track is ending.\"\"\"\n",
    "def construct_input(spectrogram, x, y):\n",
    "    if spectrogram.shape == (x,y):\n",
    "        return spectrogram\n",
    "    else: \n",
    "        ret = np.zeros((x,y))\n",
    "        \n",
    "       \n",
    "        for i in range(spectrogram.shape[0]):\n",
    "            ret[i][:spectrogram.shape[1]] = spectrogram[i][:spectrogram.shape[1]]\n",
    "        return ret\n",
    "\n",
    "def train_segment(wav, sr, length=1, hop=0.5):\n",
    "    ret = []\n",
    "    max = len(wav)/sr\n",
    "    begin = 0\n",
    "    while (begin < max):\n",
    "        adding = length if begin + length < max else max - begin\n",
    "        ret.append((begin, begin + adding))\n",
    "        begin += hop\n",
    "\n",
    "    return ret\n",
    "    \n",
    "\n",
    "def label_buffer(num_frame, label):\n",
    "    if len(label) < num_frame:\n",
    "        for i in range(num_frame- len(label)):\n",
    "            label.append([])\n",
    "    return label\n",
    "\n",
    "\n",
    "\"\"\" This function will generate all the mel_spectrogram and label pair for each audio segment of the song at index\"\"\"\n",
    "def generate_data_label(index, noised=True, num_frame=22, segment_jump = 0.5, frame_length = 2048, hop_length = 512):\n",
    "    # fetching infomation\n",
    "    (wav, sr), midi = load_wav_midi_pair(index)\n",
    "    \n",
    "    if noised:\n",
    "        wav = add_gaussian_noise(wav)\n",
    "\n",
    "    frame_time =  frame_length/sr\n",
    "    quick_sample = audio_segment_between(0, num_frame*frame_time, wav, sr)\n",
    "    x, y = extract_mel_spectrogram(quick_sample, sr).shape\n",
    "    segments = train_segment(wav, sr, length = num_frame * frame_time, hop= num_frame * frame_time/2)\n",
    "    \n",
    "    label_gen = PeekableGenerator(label_generator(num_frame, num_frame*segment_jump, hop_length, frame_length, sr, midi))\n",
    "    \n",
    "    input_datas = []\n",
    "    labels = []\n",
    "    for beg, end in segments:\n",
    "        \n",
    "        if (label_gen.has_next() is False) :\n",
    "            break\n",
    "\n",
    "        audio = audio_segment_between(beg, end, wav, sr)\n",
    "        mel_spectrogram = extract_mel_spectrogram(audio, sr, hop_length= hop_length, n_fft=frame_length)\n",
    "        input_data = construct_input(mel_spectrogram, x,y)\n",
    "        input_data = np.array(input_data)\n",
    "        input_data = np.expand_dims(input_data, axis=0)\n",
    "\n",
    "        label = label_buffer(num_frame, label_gen.__next__())\n",
    "        label = label_formater(label, beg)\n",
    "        label = np.array(label)\n",
    "\n",
    "        input_datas.append(input_data)\n",
    "        labels.append(label)\n",
    "        \n",
    "    return np.array(input_datas), np.array(labels)\n",
    "   \n",
    "def generate_data(index, num_frame=22, segment_jump = 0.5, frame_length = 2048, hop_length = 512):\n",
    "    \"\"\"This function generates the audio mel_spectrogram for the song at {index} with the defaulted training parameter\n",
    "        this function is generally used for validation process.\n",
    "    \"\"\"\n",
    "    # fetching infomation\n",
    "    (wav, sr), midi = load_wav_midi_pair(index)\n",
    "    wav, sr = load_wav_from_index(index)\n",
    "    \n",
    "    frame_time =  frame_length/sr\n",
    "    quick_sample = audio_segment_between(0, num_frame*frame_time, wav, sr)\n",
    "    x, y = extract_mel_spectrogram(quick_sample, sr).shape\n",
    "    segments = train_segment(wav, sr, length = num_frame * frame_time, hop= num_frame * frame_time/2)\n",
    "    input_datas = []\n",
    " \n",
    "    for beg, end in segments:\n",
    "        audio = audio_segment_between(beg, end, wav, sr)\n",
    "        mel_spectrogram = extract_mel_spectrogram(audio, sr, hop_length= hop_length, n_fft=frame_length)\n",
    "        input_data = construct_input(mel_spectrogram, x,y)\n",
    "        input_data = np.array(input_data)\n",
    "        input_data = np.expand_dims(input_data, axis=0)   \n",
    "        input_datas.append(input_data)\n",
    "        \n",
    "    return np.array(input_datas)\n",
    "\n",
    "\n",
    "\n",
    "def one_pass_song_train(id, index, ratio, noised=True, num_frame=22, segment_jump = 0.5, frame_length = 2048\n",
    "                        , hop_length = 512, batch_size=16 ):\n",
    "    #load model\n",
    "    \"\"\"The training is based on how many frame should be trained at a time,\n",
    "        the default setting is suited for the expriment set up above,\n",
    "        num_frame is tried to aligned it to approx 1 second of the sample\n",
    "        segment_jumping would be trying to get 50% of the sample audio\n",
    "        The dimension of the input is:\n",
    "        batchsize * channel * \n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = PianoNoteModel()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    #Letting the classification utility be more potent than the regression utility.\n",
    "    criterion = MultiTaskLoss(classification_weight=1.7, regression_weight=0.5)\n",
    "  \n",
    "    model, optimizer = load_model(model, optimizer, id)\n",
    "\n",
    "    # generate all data\n",
    "    datas, labels = generate_data_label(index, noised=noised, num_frame=num_frame, segment_jump=segment_jump, frame_length=frame_length, hop_length=hop_length)\n",
    "    datas = torch.tensor(datas, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    dataset = TensorDataset(datas, labels)\n",
    "    # Let model be in training mode\n",
    "    model.train()\n",
    "    dataloader = DataLoader(dataset, batch_size = batch_size, shuffle =True)\n",
    "    # Initialize the feedback value before training to \n",
    "    \n",
    "\n",
    "    lossval = 0.0\n",
    "    #for _, (batch_inputs, batch_labels) in progress_bar:\n",
    "    for batch_inputs, batch_labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(batch_inputs)\n",
    "\n",
    "        #  classification training\n",
    "        classification_output = outputs[..., 0:1]\n",
    "        classification_target = batch_labels[..., 0:1]\n",
    "        \n",
    "        #  regression training\n",
    "        regression_output = outputs[..., 1:3]\n",
    "        regression_target = batch_labels[...,1:3]\n",
    "        loss = criterion.forward(classification_output, classification_target, regression_output, regression_target, ratio)\n",
    "    \n",
    "        \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        lossval+= loss.item()\n",
    "\n",
    "        \n",
    "    #save model\n",
    "    save_model(model, optimizer, id)\n",
    "    return lossval\n",
    "\n",
    "def save_training_message_log(id, log):\n",
    "    # Open the file in write mode and write the string\n",
    "    path = \"models/\"+ id + \"_training_log.txt\"\n",
    "    with open(path, \"w\") as file:\n",
    "        file.write(log)\n",
    "\n",
    "def extract_ratios_from_log_file(id):\n",
    "    path = \"models/\"+ id + \"_training_log.txt\"\n",
    "    # Open the file and read its contents\n",
    "    with open(path, 'r') as file:\n",
    "        text = file.read()\n",
    "    # Regular expression to match \"Ratio of pressed is in data is <value>\"\n",
    "    pattern = r\"The ratio was trained with (\\d+\\.\\d+)\"\n",
    " \n",
    "    # Find all matching ratios in the text\n",
    "    ratios = re.findall(pattern, text)\n",
    "    \n",
    "    # Convert each ratio to a float for easier processing (optional)\n",
    "    return [float(ratio) for ratio in ratios]\n",
    "\n",
    "def loss(pc, fc):\n",
    "    return abs(1-pc) + abs(1-fc)\n",
    "\n",
    "def train_N_song_on_epoch (id, epoch,  N, back_up, ratio_update = True, ratio_load_prev = True, ratio=1.5, repetition = 2, noised=True):\n",
    "    \n",
    "    \"\"\"prev_ratio = extract_ratios_from_log_file(id)[-1]\n",
    "    if (prev_ratio != None and ratio_load_prev):\n",
    "        ratio = prev_ratio\"\"\"\n",
    "    \n",
    "    tobe_done = load_progress_index_from_csv(epoch, id)\n",
    "\n",
    "    N = min(N, len(tobe_done))\n",
    "    ## indices to be trained for the current run\n",
    "    parse_in = [tobe_done.pop() for _ in range(N)]\n",
    "\n",
    "    random.shuffle(parse_in)\n",
    "\n",
    "    progress = tqdm.tqdm(parse_in)\n",
    "\n",
    "\n",
    "    \n",
    "    counter = 0\n",
    "    trained = []\n",
    "    ## Passing in each index to train on one_pass_song_train\n",
    "    pc = 0.7928640553348053\n",
    "    fc = 0.5865201712217555\n",
    "    ratio_dummy = 1.51\n",
    "    training_message = \"\"\n",
    "    for index in progress:\n",
    "        for _ in range(repetition):\n",
    "            one_pass_song_train(id, index, ratio, noised=noised)\n",
    "        trained.append(index)\n",
    "        counter += 1\n",
    "        if (counter % 7 == 0):\n",
    "            new_pc, new_fc, _ = validation_N_songs(id, 5)\n",
    "            \n",
    "            training_message += '\\n' + f\"The ratio was trained with {ratio}. Trained {len(trained)} songs.\" + '\\n'\n",
    "            training_message += f\"pressed_corrected: {new_pc}. Frame_accuracy: {new_fc}.\" + '\\n'\n",
    "            dl = loss(new_pc, new_fc) - loss(pc, fc)\n",
    "            dr = ratio - ratio_dummy + 1e-5\n",
    "            pc = new_pc\n",
    "            fc = new_fc\n",
    "            if (ratio_update):\n",
    "                ratio_dummy = ratio\n",
    "                ratio = ratio + 0.1*(dl/dr)\n",
    "        #save this to a messaage file\n",
    "        \n",
    "        save_training_message_log(id, training_message)\n",
    "    \n",
    "\n",
    "    new_pc, new_fc, _ = validation_N_songs(id, 5)\n",
    "            \n",
    "    training_message += log + '\\n' + f\"The ratio was trained with {ratio}. Trained {len(trained)} songs.\" + '\\n' + '\\n'\n",
    "    dl = loss(new_pc, new_fc) - loss(pc, fc)\n",
    "    dr = ratio - ratio_dummy + 1e-5\n",
    "    pc = new_pc\n",
    "    fc = new_fc\n",
    "    if (ratio_update):\n",
    "        ratio_dummy = ratio\n",
    "        ratio = ratio + 0.1*(dl/dr)\n",
    "    save_training_message_log(id, training_message)\n",
    "    \n",
    "    if (len(tobe_done) == 0):\n",
    "        print(f\"Epoch {epoch} training complete.\")\n",
    "    else :\n",
    "        \n",
    "        print(f\"Epoch {epoch} have trained {N} songs, left over songs for current epoch will be saved. Need to train {len(tobe_done)}.\")\n",
    "        save_progress_index_to_csv(tobe_done, epoch, id)\n",
    "    actualPath = f\"models/{id}_piano_model.pth\"\n",
    "    print(f\"Model saved to {actualPath}\")\n",
    "\n",
    "    model = PianoNoteModel()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    #Letting the classification utility be more potent than the regression utility.\n",
    "  \n",
    "    model, optimizer = load_model(model, optimizer, id)\n",
    "\n",
    "    back_up_id = \"\" + id + back_up\n",
    "    save_model(model, optimizer, back_up_id)\n",
    "    save_training_message_log(back_up_id, training_message)\n",
    "    print(f\"back_up_id {id} + {back_up}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c448858",
   "metadata": {},
   "source": [
    "### Validation functions\n",
    "\n",
    "Use the function below to test the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56772593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validation_accuracy_check(id, index, noised=True, num_frame=22, segment_jump = 0.5, frame_length = 2048, hop_length = 512, batch_size=16):\n",
    "    \"\"\"This function returns the accuracy of the model predicting the song at index\n",
    "        arguments:\n",
    "            id: trained model id\n",
    "            index: index of the song.\n",
    "    \"\"\"\n",
    "    model = PianoNoteModel()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    #Letting the classification utility be more potent than the regression utility.\n",
    "    criterion = MultiTaskLoss(classification_weight=1.5, regression_weight=0.7)\n",
    " \n",
    "    \n",
    "    model, optimizer = load_model(model, optimizer, id)\n",
    "   \n",
    "\n",
    "    datas, labels = generate_data_label(index, num_frame=num_frame, segment_jump=segment_jump, frame_length=frame_length, hop_length=hop_length)\n",
    "    \n",
    "\n",
    "    datas = torch.tensor(datas, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    dataset = TensorDataset(datas, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    classification_correct = 0\n",
    "    classification_true_correct = 0\n",
    "    regression_error = 0\n",
    "\n",
    "    true_classified = 0\n",
    "    total_classifed = 0\n",
    "    num_classified = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for val_inputs, val_labels in dataloader:\n",
    "            outputs = model(val_inputs)\n",
    "\n",
    "            #Output separation\n",
    "            classification_outputs = outputs[:,:,:, 0:1].squeeze(-1)\n",
    "            regression_outputs = outputs[:, :, :, 1:3]\n",
    "\n",
    "            #Label separation\n",
    "            classification_labels = val_labels[:, :, :, 0:1].squeeze(-1)\n",
    "            regression_labels = val_labels[:, :, :, 1:3]\n",
    "            \n",
    "            #Calculate the classification accuracy of a frame\n",
    "            predicted_classes = (classification_outputs > 0.5).float()\n",
    "            classification_correct += (predicted_classes == classification_labels).sum().item() / model.num_mel_bins #this should be the accuracy of a frame\n",
    "\n",
    "\n",
    "            #Calculate the classification accuracy of pressed key\n",
    "            pressed_key_mask = classification_labels == 1\n",
    "            pressed_key_label = classification_labels[pressed_key_mask]\n",
    "            pressed_key_prediction = predicted_classes[pressed_key_mask]\n",
    "            true_classified += pressed_key_label.numel()\n",
    "            total_classifed += classification_labels.numel()\n",
    "            classification_true_correct += (pressed_key_label == pressed_key_prediction).sum().item()\n",
    "            \n",
    "\n",
    "            regression_error += ((regression_outputs - regression_labels) ** 2).mean().item()\n",
    "            \n",
    "            num_classified += val_labels.shape[0] * val_labels.shape[1] # counting number of frames have been classified i.e. batch_number * number of frame per segment\n",
    "            \n",
    "\n",
    "    if true_classified > 0 :\n",
    "        true_accuracy = classification_true_correct/true_classified\n",
    "    else:\n",
    "        true_accuracy = 0\n",
    "    \n",
    " \n",
    "    return true_accuracy, classification_correct/ num_classified, \n",
    "\n",
    "\"\"\"Validating N random songs from the validation set and returns the average of their accuracy in frames and pressed_notes\"\"\"\n",
    "def validation_N_songs(id, N, model=None, optimizer=None, noised=True, num_frame=22, segment_jump = 0.5, frame_length = 2048, hop_length = 512, batch_size=16):\n",
    "    if model == None and optimizer == None:\n",
    "        model = PianoNoteModel(output_size=(88, 3))\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        model, optimizer = load_model(model, optimizer, id)\n",
    "    all_indices = load_index_from_csv(\"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/validation_indicies.csv\")\n",
    "    indices_subset = random.sample(all_indices,N)\n",
    "\n",
    "    classification_correct = 0\n",
    "    classification_true_correct = 0\n",
    "    regression_error = 0\n",
    "\n",
    "    true_classified = 0\n",
    "    total_classifed = 0\n",
    "    num_classified = 0\n",
    "    with torch.no_grad():\n",
    "        for index in indices_subset:\n",
    "            datas, labels = generate_data_label(index, num_frame=num_frame, segment_jump=segment_jump, frame_length=frame_length, hop_length=hop_length)\n",
    "            datas = torch.tensor(datas, dtype=torch.float32)\n",
    "            labels = torch.tensor(labels, dtype=torch.float32)\n",
    "            dataset = TensorDataset(datas, labels)\n",
    "            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "            for val_inputs, val_labels in dataloader:\n",
    "                outputs = model(val_inputs)\n",
    "\n",
    "                #Output separation\n",
    "                classification_outputs = outputs[:,:,:, 0:1].squeeze(-1)\n",
    "                regression_outputs = outputs[:, :, :, 1:3]\n",
    "\n",
    "                #Label separation\n",
    "                classification_labels = val_labels[:, :, :, 0:1].squeeze(-1)\n",
    "                regression_labels = val_labels[:, :, :, 1:3]\n",
    "                \n",
    "                #Calculate the classification accuracy of a frame\n",
    "                predicted_classes = (classification_outputs > 0.5).float()\n",
    "                classification_correct += (predicted_classes == classification_labels).sum().item() / model.num_mel_bins #this should be the accuracy of a frame\n",
    "\n",
    "\n",
    "                #Calculate the classification accuracy of pressed key\n",
    "                pressed_key_mask = classification_labels == 1\n",
    "                pressed_key_label = classification_labels[pressed_key_mask]\n",
    "                pressed_key_prediction = predicted_classes[pressed_key_mask]\n",
    "                true_classified += pressed_key_label.numel()\n",
    "                total_classifed += classification_labels.numel()\n",
    "                classification_true_correct += (pressed_key_label == pressed_key_prediction).sum().item()\n",
    "                \n",
    "\n",
    "                regression_error += ((regression_outputs - regression_labels) ** 2).mean().item()\n",
    "                \n",
    "                num_classified += val_labels.shape[0] * val_labels.shape[1]\n",
    "    if true_classified > 0 :\n",
    "        true_accuracy = classification_true_correct/true_classified\n",
    "    else:\n",
    "        true_accuracy = 0\n",
    "    \n",
    "    return true_accuracy, classification_correct/ num_classified, true_classified/total_classifed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d5066c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b42ac20",
   "metadata": {},
   "source": [
    "### Index Processing\n",
    "\n",
    "Using the saved indices we have processed before  \n",
    "Here are the file paths  \n",
    "for training datas :traindata/maestro-v3.0.0-midi/maestro-v3.0.0/train_indicies.csv  \n",
    "for test datas :traindata/maestro-v3.0.0-midi/maestro-v3.0.0/test_indicies.csv   \n",
    "for validataion datas :traindata/maestro-v3.0.0-midi/maestro-v3.0.0/validation_indicies.csv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64563c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only run this once\n",
    "train_indices = load_index_from_csv(\"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/train_indicies.csv\")\n",
    "validation_indices = load_index_from_csv(\"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/validation_indicies.csv\")\n",
    "test_indices = load_index_from_csv(\"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/test_indicies.csv\")\n",
    "\n",
    "def initiate_train_epoch(id, epoch_Number, dataset):\n",
    "    train_indices = load_index_from_csv(dataset)\n",
    "    train_indices = randomizeing(train_indices)\n",
    "    save_progress_index_to_csv(train_indices, epoch_Number, id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc9808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the training epoch. RUN ONCE BEFORE TRAINING\n",
    "#initiate_train_epoch(0, \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/train_indicies.csv\")\n",
    "\n",
    "# Initiate the training model. RUN ONCE BEFORE TRAINING\n",
    "\"\"\"model = PianoNoteModel(output_size=(88, 3))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "save_model(model, optimizer, \"test\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f318a2c7",
   "metadata": {},
   "source": [
    "## Trainign prototype\n",
    "Experimenting training on one single sample to see if there are any progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafc3af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]C:\\Users\\youxu\\AppData\\Local\\Temp\\ipykernel_17368\\3740269339.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(actualPath)\n",
      "  7%|▋         | 7/100 [06:26<1:40:48, 65.04s/it]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "model = PianoNoteModel(output_size=(88, 3))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "initiate_train_epoch(id, 0, \"traindata/maestro-v3.0.0-midi/maestro-v3.0.0/train_indicies.csv\")\n",
    "save_model(model, optimizer, id)\"\"\"\n",
    "id = \"first_prototype_ep0\"\n",
    "train_N_song_on_epoch(id, 0, 100, \"0\", ratio_update=False, ratio_load_prev=False, ratio=1.52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d264b86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youxu\\AppData\\Local\\Temp\\ipykernel_17368\\2102600888.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(actualPath)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8317629713892277, 0.6338905264246557, 0.0314080143664382)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_N_songs(\"first_prototype_ep0\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50269a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7928640553348053"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.8249471567768212 + 0.8595249636451769 + 0.5028892540953189 + 0.7157269546822879 + 0.8758578272691959 + 0.9782381755400306)/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "570cdbbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'Classified 21758 frames of audio, classification arcuracy 0.6180057074096049, average regression error is 0.18473734000071257\\nThe true accuracy of classifying pressed_key are 0.8629531114685142\\nRatio of pressed is in data is 0.033772321988150646'\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c\n",
    "\"\"\"'Classified 21758 frames of audio, classification arcuracy 0.6180057074096049, average regression error is 0.18473734000071257\\nThe true accuracy of classifying pressed_key are 0.8629531114685142\\nRatio of pressed is in data is 0.033772321988150646'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e12fea85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.503"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weightadjsut (pc, new_pc, fc, new_fc, weight):\n",
    "    delta = 0.1 * ((new_pc - pc) - 0.7* (new_fc - fc) )\n",
    "    return weight + delta\n",
    "\"\"\"ArithmeticError\n",
    "Classified 29128 frames of audio, classification arcuracy 0.22729457441761755, average regression error is 0.16237703282094468\n",
    "The true accuracy of classifying pressed_key are 0.9963199053689952\n",
    "Ratio of pressed is in data is 0.029682857481710817\n",
    "The ratio was trained with 1.7045658300552646. Trained 84 songs.\n",
    "Classified 27456 frames of audio, classification arcuracy 0.16490310116020349, average regression error is 0.16563519005314176\n",
    "The true accuracy of classifying pressed_key are 0.999739833168019\n",
    "Ratio of pressed is in data is 0.02545353557427421\n",
    "The ratio was trained with 1.7208529809521513. Trained 91 songs.\n",
    "\"\"\"\n",
    "weightadjsut(0.22729457441761755)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a5ce42",
   "metadata": {},
   "source": [
    "# Using the model\n",
    "\n",
    "Below are functions to use for trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7d4ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_output(id, index):\n",
    "    \"\"\" This function returns the audio classification from the model {id}\n",
    "        arguments : id (model id)\n",
    "                    index (song index)\n",
    "            return: classification result\n",
    "    \"\"\"\n",
    "    model = PianoNoteModel()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    model, _ = load_model(model, optimizer, id)\n",
    "    audio_data = generate_data(index)\n",
    "\n",
    "\n",
    "    datas = torch.tensor(audio_data, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    dataloader = DataLoader(audio_data, batch_size = 16, shuffle =True)\n",
    "    \n",
    "\n",
    "\n",
    "    dataset = TensorDataset(datas, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size = batch_size, shuffle =True)\n",
    "\n",
    "    \n",
    "    progress_bar = tqdm.tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    for batch_inputs, batch_labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(dataloader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def midi_reconstruction(outputed_data):\n",
    "    \"\"\"Reconstruct the midi object from the outputed_data.\n",
    "        Arguments:\n",
    "            outputed_data: A list of outputed matrixies[batch_size * frame_number * 88 * 3]\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
